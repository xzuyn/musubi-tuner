> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Advanced configuration / é«˜åº¦ãªè¨­å®š

## Table of contents / ç›®æ¬¡

- [Using configuration files to specify training options](#using-configuration-files-to-specify-training-options--è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æŒ‡å®š)
- [How to specify `network_args`](#how-to-specify-network_args--network_argsã®æŒ‡å®šæ–¹æ³•)
- [LoRA+](#lora)
- [Select the target modules of LoRA](#select-the-target-modules-of-lora--loraã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹)
- [Save and view logs in TensorBoard format](#save-and-view-logs-in-tensorboard-format--tensorboardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§)
- [Save and view logs in wandb](#save-and-view-logs-in-wandb--wandbã§ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§)
- [FP8 weight optimization for models](#fp8-weight-optimization-for-models--ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®fp8ã¸ã®æœ€é©åŒ–)
- [PyTorch Dynamo optimization for model training](#pytorch-dynamo-optimization-for-model-training--ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã‘ã‚‹pytorch-dynamoã®æœ€é©åŒ–)
- [MagCache](#magcache)
- [Style-Friendly SNR Sampler](#style-friendly-snr-sampler)
- [Specify time step range for training](#specify-time-step-range-for-training--å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ç¯„å›²ã®æŒ‡å®š)
- [Timestep Bucketing for Uniform Sampling](#timestep-bucketing-for-uniform-sampling--å‡ä¸€ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãŸã‚ã®timestep-bucketing)
- [Schedule Free Optimizer](#schedule-free-optimizer--ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ•ãƒªãƒ¼ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶)

[Post-Hoc EMA merging for LoRA](tools.md#lora-post-hoc-ema-merging--loraã®post-hoc-emaãƒãƒ¼ã‚¸) is described in the [Tools](tools.md) document.

## Using configuration files to specify training options / è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æŒ‡å®š

Instead of specifying all training options on the command line, you can use a `.toml` configuration file to specify them. This can make it easier to manage and reuse training configurations.

Specify the configuration file with the `--config_file` option. The `.toml` extension can be omitted.

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --config_file config.toml
```

The configuration file is a TOML file that can contain any of the command-line options. The file can be organized into sections for readability, but all sections are flattened when parsed, so the section names are ignored.

<details>
<summary>æ—¥æœ¬èª</summary>

ã™ã¹ã¦ã®å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æŒ‡å®šã™ã‚‹ä»£ã‚ã‚Šã«ã€`.toml`è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’è¨­å®šã®ç®¡ç†ã‚„å†åˆ©ç”¨ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚

`--config_file`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚`.toml`æ‹¡å¼µå­ã¯çœç•¥ã§ãã¾ã™ã€‚

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --config_file config.toml
```

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã„ãšã‚Œã‹ã‚’å«ã‚€ã“ã¨ãŒã§ãã‚‹TOMLãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯èª­ã¿ã‚„ã™ã•ã®ãŸã‚ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€è§£ææ™‚ã«ã™ã¹ã¦ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãƒ•ãƒ©ãƒƒãƒˆåŒ–ã•ã‚Œã‚‹ãŸã‚ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚

</details>

### Example configuration file / è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹

```toml
# config.toml
dit = "/path/to/dit"
dataset_config = "/path/to/dataset.toml"
network_module = "networks.lora"
network_dim = 32
network_alpha = 16

[optimizer]
optimizer_type = "AdamW"
learning_rate = 1e-4

[training]
max_train_epochs = 10
save_every_n_epochs = 2
mixed_precision = "bf16"

[output]
output_dir = "/path/to/output"
output_name = "my_lora"
logging_dir = "./logs"
```

All options can be specified in the top level or within sections. When parsed, the section structure is ignored and all key-value pairs are combined into a single namespace.

Options specified on the command line will override those in the configuration file.

```bash
# This will use the config file but override the learning_rate
accelerate launch --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --config_file config --learning_rate 2e-4
```

<details>
<summary>æ—¥æœ¬èª</summary>

ã™ã¹ã¦ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã¾ãŸã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã«æŒ‡å®šã§ãã¾ã™ã€‚è§£ææ™‚ã«ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã¯ç„¡è¦–ã•ã‚Œã€ã™ã¹ã¦ã®ã‚­ãƒ¼ã¨å€¤ã®ãƒšã‚¢ãŒå˜ä¸€ã®ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ã«çµåˆã•ã‚Œã¾ã™ã€‚

ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æŒ‡å®šã•ã‚ŒãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚

```bash
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€learning_rateã‚’ä¸Šæ›¸ãã—ã¾ã™
accelerate launch --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --config_file config --learning_rate 2e-4
```

</details>

## How to specify `network_args` / `network_args`ã®æŒ‡å®šæ–¹æ³•

The `--network_args` option is an option for specifying detailed arguments to LoRA. Specify the arguments in the form of `key=value` in `--network_args`.

<details>
<summary>æ—¥æœ¬èª</summary>
`--network_args`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€LoRAã¸ã®è©³ç´°ãªå¼•æ•°ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚`--network_args`ã«ã¯ã€`key=value`ã®å½¢å¼ã§å¼•æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚
</details>

### Example / è¨˜è¿°ä¾‹

If you specify it on the command line, write as follows. / ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æŒ‡å®šã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --dit ... 
    --network_module networks.lora --network_dim 32 
    --network_args "key1=value1" "key2=value2" ...
```

If you specify it in the configuration file, write as follows. / è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§æŒ‡å®šã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```toml
network_args = ["key1=value1", "key2=value2", ...]
```

If you specify `"verbose=True"`, detailed information of LoRA will be displayed. / `"verbose=True"`ã‚’æŒ‡å®šã™ã‚‹ã¨LoRAã®è©³ç´°ãªæƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```bash
--network_args "verbose=True" "key1=value1" "key2=value2" ...
```

## LoRA+

LoRA+ is a method to improve the training speed by increasing the learning rate of the UP side (LoRA-B) of LoRA. Specify the multiplier for the learning rate. The original paper recommends 16, but adjust as needed. It seems to be good to start from around 4. For details, please refer to the [related PR of sd-scripts](https://github.com/kohya-ss/sd-scripts/pull/1233).

Specify `loraplus_lr_ratio` with `--network_args`.

<details>
<summary>æ—¥æœ¬èª</summary>

LoRA+ã¯ã€LoRAã®UPå´ï¼ˆLoRA-Bï¼‰ã®å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã“ã¨ã§å­¦ç¿’é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚å­¦ç¿’ç‡ã«å¯¾ã™ã‚‹å€ç‡ã‚’æŒ‡å®šã—ã¾ã™ã€‚å…ƒè«–æ–‡ã§ã¯16ã‚’æ¨å¥¨ã—ã¦ã„ã¾ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚4ç¨‹åº¦ã‹ã‚‰å§‹ã‚ã‚‹ã¨ã‚ˆã„ã‚ˆã†ã§ã™ã€‚è©³ç´°ã¯[sd-scriptsã®é–¢é€£PR]https://github.com/kohya-ss/sd-scripts/pull/1233)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--network_args`ã§`loraplus_lr_ratio`ã‚’æŒ‡å®šã—ã¾ã™ã€‚
</details>

### Example / è¨˜è¿°ä¾‹

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --dit ... 
    --network_module networks.lora --network_dim 32 --network_args "loraplus_lr_ratio=4" ...
```

## Select the target modules of LoRA / LoRAã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹

*This feature is highly experimental and the specification may change. / ã“ã®æ©Ÿèƒ½ã¯ç‰¹ã«å®Ÿé¨“çš„ãªã‚‚ã®ã§ã€ä»•æ§˜ã¯å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚*

By specifying `exclude_patterns` and `include_patterns` with `--network_args`, you can select the target modules of LoRA.

`exclude_patterns` excludes modules that match the specified pattern. `include_patterns` targets only modules that match the specified pattern.

Specify the values as a list. For example, `"exclude_patterns=[r'.*single_blocks.*', r'.*double_blocks\.[0-9]\..*']"`.

The pattern is a regular expression for the module name. The module name is in the form of `double_blocks.0.img_mod.linear` or `single_blocks.39.modulation.linear`. The regular expression is not a partial match but a complete match.

The patterns are applied in the order of `exclude_patterns`â†’`include_patterns`. By default, the Linear layers of `img_mod`, `txt_mod`, and `modulation` of double blocks and single blocks are excluded.

(`.*(img_mod|txt_mod|modulation).*` is specified.)

<details>
<summary>æ—¥æœ¬èª</summary>

`--network_args`ã§`exclude_patterns`ã¨`include_patterns`ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€LoRAã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

`exclude_patterns`ã¯ã€æŒ‡å®šã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é™¤å¤–ã—ã¾ã™ã€‚`include_patterns`ã¯ã€æŒ‡å®šã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã¾ã™ã€‚

å€¤ã¯ã€ãƒªã‚¹ãƒˆã§æŒ‡å®šã—ã¾ã™ã€‚`"exclude_patterns=[r'.*single_blocks.*', r'.*double_blocks\.[0-9]\..*']"`ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã«å¯¾ã™ã‚‹æ­£è¦è¡¨ç¾ã§ã™ã€‚ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã¯ã€ãŸã¨ãˆã°`double_blocks.0.img_mod.linear`ã‚„`single_blocks.39.modulation.linear`ã®ã‚ˆã†ãªå½¢å¼ã§ã™ã€‚æ­£è¦è¡¨ç¾ã¯éƒ¨åˆ†ä¸€è‡´ã§ã¯ãªãå®Œå…¨ä¸€è‡´ã§ã™ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€`exclude_patterns`â†’`include_patterns`ã®é †ã§é©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€double blocksã¨single blocksã®Linearå±¤ã®ã†ã¡ã€`img_mod`ã€`txt_mod`ã€`modulation`ãŒé™¤å¤–ã•ã‚Œã¦ã„ã¾ã™ã€‚

ï¼ˆ`.*(img_mod|txt_mod|modulation).*`ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ï¼‰
</details>

### Example / è¨˜è¿°ä¾‹

Only the modules of double blocks / double blocksã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆ:

```bash
--network_args "exclude_patterns=[r'.*single_blocks.*']"
```

Only the modules of single blocks from the 10th / single blocksã®10ç•ªç›®ä»¥é™ã®Linearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆ:

```bash
--network_args "exclude_patterns=[r'.*']" "include_patterns=[r'.*single_blocks\.\d{2}\.linear.*']"
```

## Save and view logs in TensorBoard format / TensorBoardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§

Specify the folder to save the logs with the `--logging_dir` option. Logs in TensorBoard format will be saved.

For example, if you specify `--logging_dir=logs`, a `logs` folder will be created in the working folder, and logs will be saved in the date folder inside it.

Also, if you specify the `--log_prefix` option, the specified string will be added before the date. For example, use `--logging_dir=logs --log_prefix=lora_setting1_` for identification.

To view logs in TensorBoard, open another command prompt and activate the virtual environment. Then enter the following in the working folder.

```powershell
tensorboard --logdir=logs
```

(tensorboard installation is required.)

Then open a browser and access http://localhost:6006/ to display it.

<details>
<summary>æ—¥æœ¬èª</summary>
`--logging_dir`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ãƒ­ã‚°ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚TensorBoardå½¢å¼ã®ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

ãŸã¨ãˆã°`--logging_dir=logs`ã¨æŒ‡å®šã™ã‚‹ã¨ã€ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã«logsãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã€ãã®ä¸­ã®æ—¥æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

ã¾ãŸ`--log_prefix`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨ã€æ—¥æ™‚ã®å‰ã«æŒ‡å®šã—ãŸæ–‡å­—åˆ—ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚`--logging_dir=logs --log_prefix=lora_setting1_`ãªã©ã¨ã—ã¦è­˜åˆ¥ç”¨ã«ãŠä½¿ã„ãã ã•ã„ã€‚

TensorBoardã§ãƒ­ã‚°ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€åˆ¥ã®ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ãã€ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹ã«ã—ã¦ã‹ã‚‰ã€ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã§ä»¥ä¸‹ã®ã‚ˆã†ã«å…¥åŠ›ã—ã¾ã™ã€‚

```powershell
tensorboard --logdir=logs
```

ï¼ˆtensorboardã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ï¼‰

ãã®å¾Œãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ãã€http://localhost:6006/ ã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
</details>

## Save and view logs in wandb / wandbã§ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§

`--log_with wandb` option is available to save logs in wandb format. `tensorboard` or `all` is also available. The default is `tensorboard`.

Specify the project name with `--log_tracker_name` when using wandb.

<details>
<summary>æ—¥æœ¬èª</summary>
`--log_with wandb`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨wandbå½¢å¼ã§ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚`tensorboard`ã‚„`all`ã‚‚æŒ‡å®šå¯èƒ½ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`tensorboard`ã§ã™ã€‚

wandbã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`--log_tracker_name`ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
</details>

## FP8 weight optimization for models / ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®FP8ã¸ã®æœ€é©åŒ–

The `--fp8_scaled` option performs an offline optimization pass that rewrites selected Linear weights into FP8 (E4M3) with block-wise scaling. Compared with the legacy `--fp8` cast, it reduces VRAM usage while maintaining relatively high precision.

From v0.2.12, block-wise scaling is supported instead of per-tensor scaling, allowing for higher precision quantization.

This flow dequantizes back the weights to the FP16/BF16/FP32 weights during the forward path, and computes in FP16/BF16/FP32. The shared routines live in `src/musubi_tuner/modules/fp8_optimization_utils.py` and are wired into the Wan2.x, FramePack, FLUX.1 Kontext, and Qwen-Image pipelines (except HunyuanVideo, which `--fp8_scaled` is not supported).

Acknowledgments: This idea is based on the [implementation](https://github.com/Tencent/HunyuanVideo/blob/7df4a45c7e424a3f6cd7d653a7ff1f60cddc1eb1/hyvideo/modules/fp8_optimization.py) of [HunyuanVideo](https://github.com/Tencent/HunyuanVideo). The selection of high-precision modules is referenced from the [implementation](https://github.com/tdrussell/diffusion-pipe/blob/407c04fdae1c9ab5e67b54d33bef62c3e0a8dbc7/models/wan.py) of [diffusion-pipe](https://github.com/tdrussell/diffusion-pipe). I would like to thank these repositories.

<details>
<summary>æ—¥æœ¬èª</summary>

`--fp8_scaled` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€å¯¾è±¡ã® Linear å±¤ã®é‡ã¿ã‚’ã€blockã”ã¨ã«é©åˆ‡ãªå€ç‡ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãŸ FP8 (E4M3) ã«æ›¸ãæ›ãˆã‚‹å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚å¾“æ¥ã® `--fp8` ã«ã‚ˆã‚‹å˜ç´”ãªã‚­ãƒ£ã‚¹ãƒˆã¨æ¯”ã¹ã¦ã€å…ƒã®ç²¾åº¦ã‚’æ¯”è¼ƒçš„ä¿ã£ãŸã¾ã¾ VRAM ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

v0.2.12ã‹ã‚‰ã€ãƒ†ãƒ³ã‚½ãƒ«ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§ã¯ãªãã€ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«å¯¾å¿œã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šé«˜ã„ç²¾åº¦ã§ã®é‡å­åŒ–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

forward ã®è¨ˆç®—ã¯ã€é€†é‡å­åŒ–ã‚’è¡Œãªã£ãŸé‡ã¿ã§ FP16/BF16 ã§è¡Œã‚ã‚Œã¾ã™ã€‚å…±é€šãƒ«ãƒ¼ãƒãƒ³ã¯ `src/musubi_tuner/modules/fp8_optimization_utils.py` ã«ã‚ã‚Šã€Wan 2.xãƒ»FramePackãƒ»FLUX.1 Kontextãƒ»Qwen-Image ã®å„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§åˆ©ç”¨ã•ã‚Œã¾ã™ï¼ˆHunyuanVideo ã«ã¤ã„ã¦ã¯ `--fp8_scaled` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ç„¡åŠ¹ã§ã™ï¼‰ã€‚

ã“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€[HunyuanVideo](https://github.com/Tencent/HunyuanVideo) ã® [å®Ÿè£…](https://github.com/Tencent/HunyuanVideo/blob/7df4a45c7e424a3f6cd7d653a7ff1f60cddc1eb1/hyvideo/modules/fp8_optimization.py) ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚é«˜ç²¾åº¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é¸å®šã¯ã€[diffusion-pipe](https://github.com/tdrussell/diffusion-pipe) ã® [å®Ÿè£…](https://github.com/tdrussell/diffusion-pipe/blob/407c04fdae1c9ab5e67b54d33bef62c3e0a8dbc7/models/wan.py) ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒªã«æ„Ÿè¬ã—ã¾ã™ã€‚

</details>

### Usage summary / ä½¿ã„æ–¹ã®ã¾ã¨ã‚

- Inference: add `--fp8` and `--fp8_scaled` when running `wan_generate_video.py`, `fpack_generate_video.py`, `flux_kontext_generate_image.py`, or `qwen_image_generate_image.py`. HunyuanVideo continues to rely on `--fp8`/`--fp8_fast` without scaled weights.
- Training: specify `--fp8_base --fp8_scaled` in `wan_train_network.py`, `fpack_train_network.py`,`flux_kontext_train_network.py` and `qwen_image_train_network.py`; the trainers enforce this pairing.
- Input checkpoints must be FP16/BF16; pre-quantized FP8 weights cannot be re-optimized.
- LoRA / LyCORIS weights are merged before quantization, so no additional steps are required.

<details>
<summary>æ—¥æœ¬èª</summary>

- æ¨è«–ã§ã¯ `wan_generate_video.py`ã€`fpack_generate_video.py`ã€`flux_kontext_generate_image.py`ã€`qwen_image_generate_image.py` ã‚’å®Ÿè¡Œã™ã‚‹éš›ã« `--fp8` ã¨ `--fp8_scaled` ã‚’ä½µç”¨ã—ã¦ãã ã•ã„ã€‚HunyuanVideo ã¯å¼•ãç¶šã`--fp8` / `--fp8_fast` ã‚’ä½¿ç”¨ã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä»˜ãé‡ã¿ã¯æœªå¯¾å¿œã§ã™ã€‚
- å­¦ç¿’ã§ã¯ `wan_train_network.py`ã€`fpack_train_network.py`ã€`flux_kontext_train_network.py` ã§ `--fp8_base --fp8_scaled` ã‚’æŒ‡å®šã—ã¾ã™ã€‚
- èª­ã¿è¾¼ã‚€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ FP16/BF16 ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚ã‚‰ã‹ã˜ã‚ FP8 åŒ–ã•ã‚ŒãŸé‡ã¿ã¯å†æœ€é©åŒ–ã§ãã¾ã›ã‚“ã€‚
- LoRA / LyCORIS ã®é‡ã¿ã¯é‡å­åŒ–ã®å‰ã«è‡ªå‹•ã§ãƒãƒ¼ã‚¸ã•ã‚Œã‚‹ãŸã‚ã€è¿½åŠ ä½œæ¥­ã¯ä¸è¦ã§ã™ã€‚

</details>

### Implementation highlights / å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ


When `--fp8_scaled` flag is enabled, the loader loads the base weights in FP16/BF16, merges optional LoRA or LyCORIS, and then emits FP8 weights plus matching block-wise `.scale_weight` buffers for the targeted layers. The patched forward either dequantizes back to the original dtype on demand for computation.

The current scripts in this repository use FP8 E4M3 format and block-wise quantization, but the implementation supports:

- Implements FP8 (E4M3 or E5M2) weight quantization for Linear layers
- Supports multiple quantization modes: tensor-wise, channel-wise, and block-wise quantization described below
- Block-wise quantization provides better precision by using granular scaling with configurable block size (default: 64)
- Reduces VRAM requirements by using 8-bit weights for storage (slightly increased compared to existing `--fp8` `--fp8_base` options)
- Quantizes weights to FP8 format with appropriate scaling instead of simple cast to FP8
- Applies monkey patching to Linear layers for transparent dequantization during computation
- Maintains computational precision by dequantizing to original precision (FP16/BF16) during forward pass
- Preserves important weights for example norm, embedding, modulation in FP16/BF16 format (fewer exclusions than previous versions)

For quantization and precision discussion, see also [Discussion #564](https://github.com/kohya-ss/musubi-tuner/discussions/564).

Note: Testing for quantization other than E4M3/block-wise is limited, so please be cautious if you plan to use the code in other projects.

<details>
<summary>æ—¥æœ¬èª</summary>

`--fp8_scaled` ãƒ•ãƒ©ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€ãƒ­ãƒ¼ãƒ€ãƒ¼ã¯ã¾ãšãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é‡ã¿ã‚’ FP16/BF16 ã®ã¾ã¾èª­ã¿è¾¼ã¿ã€å¿…è¦ã«å¿œã˜ã¦ LoRA ã‚„ LyCORIS ã‚’ãƒãƒ¼ã‚¸ã—ãŸå¾Œã€å¯¾è±¡å±¤ã®é‡ã¿ã‚’ FP8 ã®é‡ã¿ã¨ã€ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã® `.scale_weight` ãƒãƒƒãƒ•ã‚¡ã¸å¤‰æ›ã—ã¾ã™ã€‚forward ã§ã¯ã“ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦å…ƒã®ç²¾åº¦ã¸å‹•çš„ã«é€†é‡å­åŒ–ã—è¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã®ç¾åœ¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€é‡å­åŒ–ã¯FP8 E4M3å½¢å¼ã€ãƒ–ãƒ­ãƒƒã‚¯å˜ä½é‡å­åŒ–ãŒç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ãŒã€å®Ÿè£…ã¨ã—ã¦ã¯ä»¥ä¸‹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼š

- Linearå±¤ã®FP8ï¼ˆE4M3ã¾ãŸã¯E5M2ï¼‰é‡ã¿é‡å­åŒ–ã‚’å®Ÿè£…
- è¤‡æ•°ã®é‡å­åŒ–ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼šãƒ†ãƒ³ã‚½ãƒ«å˜ä½ã€ãƒãƒ£ãƒãƒ«å˜ä½ã€ãƒ–ãƒ­ãƒƒã‚¯å˜ä½é‡å­åŒ–
- ãƒ–ãƒ­ãƒƒã‚¯å˜ä½é‡å­åŒ–ã¯æŒ‡å®šã—ãŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š64ï¼‰ã§ã®ç´°ç²’åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã‚ˆã‚Šé«˜ã„ç²¾åº¦ã‚’æä¾›
- 8ãƒ“ãƒƒãƒˆã®é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ï¼ˆæ—¢å­˜ã®`--fp8` `--fp8_base` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«æ¯”ã¹ã¦å¾®å¢—ï¼‰
- å˜ç´”ãªFP8ã¸ã®castã§ã¯ãªãã€é©åˆ‡ãªå€¤ã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦é‡ã¿ã‚’FP8å½¢å¼ã«é‡å­åŒ–
- Linearå±¤ã«monkey patchingã‚’é©ç”¨ã—ã€è¨ˆç®—æ™‚ã«é€éçš„ã«é€†é‡å­åŒ–
- forwardæ™‚ã«å…ƒã®ç²¾åº¦ï¼ˆFP16/BF16ï¼‰ã«é€†é‡å­åŒ–ã—ã¦è¨ˆç®—ç²¾åº¦ã‚’ç¶­æŒ
- ç²¾åº¦ãŒé‡è¦ãªé‡ã¿ã€ãŸã¨ãˆã°normã‚„embeddingã€modulationã¯ã€FP16/BF16ã®ã¾ã¾ä¿æŒï¼ˆå¾“æ¥ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚ˆã‚Šé™¤å¤–å¯¾è±¡ã‚’å‰Šæ¸›ï¼‰

é‡å­åŒ–ã¨ç²¾åº¦ã«ã¤ã„ã¦ã¯[Discussion #564](https://github.com/kohya-ss/musubi-tuner/discussions/564)ã‚‚å‚ç…§ã—ã¦ãã ã•ã„ã€‚

â€»E4M3/ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ä»¥å¤–ã®é‡å­åŒ–ã®ãƒ†ã‚¹ãƒˆã¯ä¸ååˆ†ã§ã™ã®ã§ã€ã‚³ãƒ¼ãƒ‰ã‚’ä»–ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§åˆ©ç”¨ã™ã‚‹å ´åˆç­‰ã«ã¯æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

</details>

### Quantization modes / é‡å­åŒ–ãƒ¢ãƒ¼ãƒ‰

The current implementation supports three quantization modes:

- **Block-wise quantization (default)**: Divides weight matrices into blocks of configurable size (default: 64) and calculates separate scale factors for each block. Provides the best precision but requires more memory for scale storage.
- **Channel-wise quantization**: Calculates scale factors per output channel (row). Balances precision and memory usage.
- **Tensor-wise quantization**: Uses a single scale factor for the entire weight tensor. Lowest memory usage but may have reduced precision for some weights.

The implementation automatically falls back to simpler modes when block-wise quantization is not feasible (e.g., when weight dimensions are not divisible by block size).

<details>
<summary>æ—¥æœ¬èª</summary>

ç¾åœ¨ã®å®Ÿè£…ã§ã¯3ã¤ã®é‡å­åŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼š

- **ãƒ–ãƒ­ãƒƒã‚¯å˜ä½é‡å­åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰**ï¼šé‡ã¿è¡Œåˆ—ã‚’è¨­å®šå¯èƒ½ãªã‚µã‚¤ã‚ºã®ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š64ï¼‰ã«åˆ†å‰²ã—ã€å„ãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã—ã¦å€‹åˆ¥ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ã‚’è¨ˆç®—ã—ã¾ã™ã€‚æœ€é«˜ã®ç²¾åº¦ã‚’æä¾›ã—ã¾ã™ãŒã€ã‚¹ã‚±ãƒ¼ãƒ«ä¿å­˜ã«ã‚ˆã‚Šè¿½åŠ ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã§ã™ã€‚
- **ãƒãƒ£ãƒãƒ«å˜ä½é‡å­åŒ–**ï¼šå‡ºåŠ›ãƒãƒ£ãƒãƒ«ï¼ˆè¡Œï¼‰ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ç²¾åº¦ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚Šã¾ã™ã€‚
- **ãƒ†ãƒ³ã‚½ãƒ«å˜ä½é‡å­åŒ–**ï¼šé‡ã¿ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã«å¯¾ã—ã¦å˜ä¸€ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æœ€ã‚‚å°‘ãªã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã§ã™ãŒã€ä¸€éƒ¨ã®é‡ã¿ã§ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

å®Ÿè£…ã§ã¯ã€ãƒ–ãƒ­ãƒƒã‚¯å˜ä½é‡å­åŒ–ãŒå®Ÿè¡Œä¸å¯èƒ½ãªå ´åˆï¼ˆé‡ã¿æ¬¡å…ƒãŒãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã§å‰²ã‚Šåˆ‡ã‚Œãªã„å ´åˆãªã©ï¼‰ã€è‡ªå‹•çš„ã«ã‚ˆã‚Šå˜ç´”ãªãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚

</details>

 ## PyTorch Dynamo optimization for model training / ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã‘ã‚‹PyTorch Dynamoã®æœ€é©åŒ–

The PyTorch Dynamo options are now available to optimize the training process. PyTorch Dynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster by using TorchInductor, a deep learning compiler. This integration allows for potential speedups in training while maintaining model accuracy.

[PR #215](https://github.com/kohya-ss/musubi-tuner/pull/215) added this feature.

Specify the `--dynamo_backend` option to enable Dynamo optimization with one of the available backends from the `DynamoBackend` enum.

Additional options allow for fine-tuning the Dynamo behavior:
- `--dynamo_mode`: Controls the optimization strategy
- `--dynamo_fullgraph`: Enables fullgraph mode for potentially better optimization
- `--dynamo_dynamic`: Enables dynamic shape handling

The `--dynamo_dynamic` option has been reported to have many problems based on the validation in PR #215.

### Available options:

```
--dynamo_backend {NO, INDUCTOR, NVFUSER, CUDAGRAPHS, CUDAGRAPHS_FALLBACK, etc.}
    Specifies the Dynamo backend to use (default is NO, which disables Dynamo)

--dynamo_mode {default, reduce-overhead, max-autotune}
    Specifies the optimization mode (default is 'default')
    - 'default': Standard optimization
    - 'reduce-overhead': Focuses on reducing compilation overhead
    - 'max-autotune': Performs extensive autotuning for potentially better performance

--dynamo_fullgraph
    Flag to enable fullgraph mode, which attempts to capture and optimize the entire model graph

--dynamo_dynamic
    Flag to enable dynamic shape handling for models with variable input shapes
```

### Usage example:

```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode default
```

For more aggressive optimization:
```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode max-autotune --dynamo_fullgraph
```

Note: The best combination of options may depend on your specific model and hardware. Experimentation may be necessary to find the optimal configuration.

<details>
<summary>æ—¥æœ¬èª</summary>
PyTorch Dynamoã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚PyTorch Dynamoã¯ã€TorchInductorï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€å¤‰æ›´ã‚’åŠ ãˆã‚‹ã“ã¨ãªãPyTorchãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã®Pythonãƒ¬ãƒ™ãƒ«ã®JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã§ã™ã€‚ã“ã®çµ±åˆã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ç¶­æŒã—ãªãŒã‚‰å­¦ç¿’ã®é«˜é€ŸåŒ–ãŒæœŸå¾…ã§ãã¾ã™ã€‚

[PR #215](https://github.com/kohya-ss/musubi-tuner/pull/215) ã§è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚

`--dynamo_backend`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ã€`DynamoBackend`åˆ—æŒ™å‹ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ä¸€ã¤ã‚’é¸æŠã™ã‚‹ã“ã¨ã§ã€Dynamoæœ€é©åŒ–ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚

è¿½åŠ ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€Dynamoã®å‹•ä½œã‚’å¾®èª¿æ•´ã§ãã¾ã™ï¼š
- `--dynamo_mode`ï¼šæœ€é©åŒ–æˆ¦ç•¥ã‚’åˆ¶å¾¡ã—ã¾ã™
- `--dynamo_fullgraph`ï¼šã‚ˆã‚Šè‰¯ã„æœ€é©åŒ–ã®å¯èƒ½æ€§ã®ãŸã‚ã«ãƒ•ãƒ«ã‚°ãƒ©ãƒ•ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã—ã¾ã™
- `--dynamo_dynamic`ï¼šå‹•çš„å½¢çŠ¶å‡¦ç†ã‚’æœ‰åŠ¹ã«ã—ã¾ã™

PR #215ã§ã®æ¤œè¨¼ã«ã‚ˆã‚‹ã¨ã€`--dynamo_dynamic`ã«ã¯å•é¡ŒãŒå¤šã„ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

__åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼š__

```
--dynamo_backend {NO, INDUCTOR, NVFUSER, CUDAGRAPHS, CUDAGRAPHS_FALLBACK, ãªã©}
    ä½¿ç”¨ã™ã‚‹Dynamoãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯NOã§ã€Dynamoã‚’ç„¡åŠ¹ã«ã—ã¾ã™ï¼‰

--dynamo_mode {default, reduce-overhead, max-autotune}
    æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'default'ï¼‰
    - 'default'ï¼šæ¨™æº–çš„ãªæœ€é©åŒ–
    - 'reduce-overhead'ï¼šã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹
    - 'max-autotune'ï¼šã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã«åºƒç¯„ãªè‡ªå‹•èª¿æ•´ã‚’å®Ÿè¡Œ

--dynamo_fullgraph
    ãƒ•ãƒ«ã‚°ãƒ©ãƒ•ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãƒ•ãƒ©ã‚°ã€‚ãƒ¢ãƒ‡ãƒ«ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦æœ€é©åŒ–ã—ã‚ˆã†ã¨ã—ã¾ã™

--dynamo_dynamic
    å¯å¤‰å…¥åŠ›å½¢çŠ¶ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®å‹•çš„å½¢çŠ¶å‡¦ç†ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãƒ•ãƒ©ã‚°
```

__ä½¿ç”¨ä¾‹ï¼š__

```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode default
```

ã‚ˆã‚Šç©æ¥µçš„ãªæœ€é©åŒ–ã®å ´åˆï¼š
```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode max-autotune --dynamo_fullgraph
```

æ³¨æ„ï¼šæœ€é©ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®çµ„ã¿åˆã‚ã›ã¯ã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ä¾å­˜ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚æœ€é©ãªæ§‹æˆã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«å®Ÿé¨“ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
</details>

## MagCache

The following is quoted from the [MagCache github repository](https://github.com/Zehong-Ma/MagCache) "Magnitude-aware Cache (MagCache) for Video Diffusion Models":

> We introduce Magnitude-aware Cache (MagCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps based on the robust magnitude observations, thereby accelerating the inference. MagCache works well for Video Diffusion Models, Image Diffusion models. 

We have implemented the MagCache feature in Musubi Tuner. Some of the code is based on the MagCache repository. It is available for `fpack_generate_video.py` for now.

### Usage

1. Calibrate the mag ratios
   - Run the inference script as normal, but with the `--magcache_calibration` option to calibrate the mag ratios. You will get a following output:

   ```
   INFO:musubi_tuner.fpack_generate_video:Copy and paste following values to --magcache_mag_ratios argument to use them:
   1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
   ```
   - It is recommended to run the calibration with your custom prompt and model.
   - If you inference the multi-section video, you will get the mag ratios for each section. You can use the one of the sections or average them.

2. Use the mag ratios
   - Run the inference script with the `--magcache_mag_ratios` option to use the mag ratios. For example:

   ```bash
   python fpack_generate_video.py --magcache_mag_ratios 1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
   ```

   - Specify `--magcache_mag_ratios 0` to use the default mag ratios from the MagCache repository.
   - It is recommended to use the same steps as the calibration. If the steps are different, the mag ratios is interpolated to the specified steps. 
   - You can also specify the `--magcache_retention_ratio`, `--magcache_threshold`, and `--magcache_k` options to control the MagCache behavior. The default values are 0.2, 0.24, and 6, respectively (same as the MagCache repository).

    ```bash
    python fpack_generate_video.py --magcache_retention_ratio 0.2 --magcache_threshold 0.24 --magcache_k 6
    ```

    - The `--magcache_retention_ratio` option controls the ratio of the steps not to cache. For example, if you set it to 0.2, the first 20% of the steps will not be cached. The default value is 0.2.
    - The `--magcache_threshold` option controls the threshold whether to use the cached output or not. If the accumulated error is less than the threshold, the cached output will be used. The default value is 0.24.
        - The error is calculated by the accumulated error multiplied by the mag ratio.
    - The `--magcache_k` option controls the number of steps to use for the cache. The default value is 6, which means the consecutive 6 steps will be used for the cache. The default value 6 is recommended for 50 steps, so you may want to lower it for smaller number of steps.

### Generated video example

Using F1-model, without MagCache, approximately 90 seconds are required to generate single section video with 25 steps (without VAE decoding) in my environment.

https://github.com/user-attachments/assets/30b8d05e-9bd6-42bf-997f-5ba5b3dde876

With MagCache, default settings, approximately 30 seconds are required to generate with the same settings.

https://github.com/user-attachments/assets/080076ea-4088-443c-8138-4eeb00694ec5

With MagCache, `--magcache_retention_ratio 0.2 --magcache_threshold 0.12 --magcache_k 3`, approximately 35 seconds are required to generate with the same settings.

https://github.com/user-attachments/assets/27d6c7ff-e3db-4c52-8668-9a887441acef

<details>
<summary>æ—¥æœ¬èª</summary>

ä»¥ä¸‹ã¯ã€[MagCache githubãƒªãƒã‚¸ãƒˆãƒª](https://github.com/Zehong-Ma/MagCache) "Magnitude-aware Cache (MagCache) for Video Diffusion Models"ã‹ã‚‰ã®å¼•ç”¨ã®æ‹™è¨³ã§ã™ï¼š

> Magnitude-aware Cache (MagCache)ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€å …ç‰¢ãªãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰è¦³æ¸¬ã«åŸºã¥ã„ã¦ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—é–“ã®ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®å¤‰å‹•å·®ã‚’æ¨å®šãŠã‚ˆã³æ´»ç”¨ã—ã€æ¨è«–ã‚’åŠ é€Ÿã—ã¾ã™ã€‚MagCacheã¯ã€ãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã€ç”»åƒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«é©ã—ã¦ã„ã¾ã™ã€‚

Musubi Tunerã«MagCacheæ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚ä¸€éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã¯MagCacheãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’åŸºã«ã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã¯`fpack_generate_video.py`ã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

### ä½¿ç”¨æ–¹æ³•

1. mag_ratiosã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   - `--magcache_calibration`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ã€ãã‚Œä»¥å¤–ã¯é€šå¸¸é€šã‚Šæ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã€mag ratiosã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

   ```
   INFO:musubi_tuner.fpack_generate_video:Copy and paste following values to --magcache_mag_ratios argument to use them:
   1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
   ```
   - ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
   - è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ“ãƒ‡ã‚ªã‚’æ¨è«–ã™ã‚‹å ´åˆã€å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®mag ratiosãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚ã©ã‚Œã‹ä¸€ã¤ã€ã¾ãŸã¯ãã‚Œã‚‰ã‚’å¹³å‡ã—ãŸå€¤ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚

2. mag ratiosã®ä½¿ç”¨
   - `--magcache_mag_ratios`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§mag ratiosã‚’æŒ‡å®šã—ã¦æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ä¾‹ï¼š

   ```bash
    python fpack_generate_video.py --magcache_mag_ratios 1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
    ```

    - `--magcache_mag_ratios 0`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€MagCacheãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®mag ratiosãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
    - mag ratiosã®æ•°ã¯ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸæ™‚ã¨åŒã˜ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒç•°ãªã‚‹å ´åˆã€mag ratiosã¯æŒ‡å®šã•ã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—æ•°ã«åˆã†ã‚ˆã†ã«è£œé–“ã•ã‚Œã¾ã™ã€‚
    - `--magcache_retention_ratio`, `--magcache_threshold`, `--magcache_k`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦MagCacheã®å‹•ä½œã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0.2ã€0.24ã€6ã§ã™ï¼ˆMagCacheãƒªãƒã‚¸ãƒˆãƒªã¨åŒã˜ã§ã™ï¼‰ã€‚
    
     ```bash
    python fpack_generate_video.py --magcache_retention_ratio 0.2 --magcache_threshold 0.24 --magcache_k 6
    ```

    - `--magcache_retention_ratio`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²åˆã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€0.2ã«è¨­å®šã™ã‚‹ã¨ã€æœ€åˆã®20%ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0.2ã§ã™ã€‚
    - `--magcache_threshold`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå‡ºåŠ›ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®é–¾å€¤ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ç´¯ç©èª¤å·®ãŒã“ã®é–¾å€¤æœªæº€ã®å ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå‡ºåŠ›ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0.24ã§ã™ã€‚
        - èª¤å·®ã¯ã€ç´¯ç©èª¤å·®ã«mag ratioã‚’æ›ã‘ãŸã‚‚ã®ã¨ã—ã¦è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
    - `--magcache_k`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯6ã§ã€ã“ã‚Œã¯é€£ç¶šã™ã‚‹6ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤6ã¯æã‚‰ã50ã‚¹ãƒ†ãƒƒãƒ—ã®å ´åˆã®æ¨å¥¨å€¤ã®ãŸã‚ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå°‘ãªã„å ´åˆã¯æ¸›ã‚‰ã™ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚

ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«ã¯è‹±èªã§ã®èª¬æ˜ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

## Style-Friendly SNR Sampler

This sampler is based on the paper [Style-Friendly SNR Sampler for Style-Driven Generation](https://arxiv.org/abs/2411.14793). The paper argues that stylistic features in diffusion models are predominantly learned at high noise levels. This sampler biases the noise level (timestep) sampling towards these higher noise levels, which can significantly improve the model's ability to learn and reproduce specific styles.

This feature is enabled by specifying `--timestep_sampling`.

<details>
<summary>æ—¥æœ¬èª</summary>

ã“ã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¯ã€è«–æ–‡ã€Œ[Style-Friendly SNR Sampler for Style-Driven Generation](https://arxiv.org/abs/2411.14793)ã€ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚ã“ã®è«–æ–‡ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ã‚¹ã‚¿ã‚¤ãƒ«ç‰¹å¾´ã¯ã€ä¸»ã«ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ãŒé«˜ã„é ˜åŸŸã§å­¦ç¿’ã•ã‚Œã‚‹ã¨ä¸»å¼µã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¯ã€ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æ„å›³çš„ã«é«˜ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«å´ã«åã‚‰ã›ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒç‰¹å®šã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å­¦ç¿’ãƒ»å†ç¾ã™ã‚‹èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã“ã®æ©Ÿèƒ½ã¯ `--timestep_sampling` ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§æœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚
</details>

### `logsnr` Sampler

This is a direct implementation of the sampler proposed in the paper. It samples the log-SNR value from a normal distribution. By setting a low mean and a large standard deviation, it focuses the training on high-noise levels crucial for style learning.

To use this, specify `logsnr` for `--timestep_sampling`. You can also configure the mean and standard deviation of the log-SNR distribution with `--logit_mean` and `--logit_std`.

The paper recommends `logit_mean=-6.0` and `logit_std` of 2.0 or 3.0.

```bash
accelerate launch ... \
    --timestep_sampling logsnr \
    --logit_mean -6.0 \
    --logit_std 2.0
```

Following is the distribution of the logsnr sampler:

![Distribution of logsnr sampler](logsnr_distribution.png)

<details>
<summary>æ—¥æœ¬èª</summary>

è«–æ–‡ã§ææ¡ˆã•ã‚ŒãŸé€šã‚Šã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®å®Ÿè£…ã§ã™ã€‚log-SNRå€¤ã‚’æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚ä½ã„å¹³å‡å€¤ã¨å¤§ããªæ¨™æº–åå·®ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€ã‚¹ã‚¿ã‚¤ãƒ«ã®å­¦ç¿’ã«ä¸å¯æ¬ ãªé«˜ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«é ˜åŸŸã«å­¦ç¿’ã‚’é›†ä¸­ã•ã›ã¾ã™ã€‚

ä½¿ç”¨ã™ã‚‹ã«ã¯ã€`--timestep_sampling` ã« `logsnr` ã‚’æŒ‡å®šã—ã¾ã™ã€‚ã¾ãŸã€`--logit_mean` ã¨ `--logit_std` ã§log-SNRåˆ†å¸ƒã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨­å®šã§ãã¾ã™ã€‚

è«–æ–‡ã§ã¯ `logit_mean=-6.0`ã€`logit_std` ã¯2.0ã¾ãŸã¯3.0ãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

</details>


### `qinglong_flux` and `qinglong_qwen` Sampler (Hybrid Sampler)

This is a hybrid sampling method that combines three different samplers to balance style learning, model stability, and detail preservation. It is an experimental feature inspired by the Style-Friendly SNR Sampler. It was proposed by sdbds (Qing Long) in PR [#407](https://github.com/kohya-ss/musubi-tuner/pull/407). 

In each training step, one of the following samplers is chosen for each sample in the batch based on a predefined ratio:

1.  **flux_shift or qwen_shift (80%)**: The standard sampler for high-resolution models. Focuses on overall stability.
2.  **logsnr (7.5%)**: The Style-Friendly sampler. Focuses on style learning.
3.  **logsnr2 (12.5%)**: A sampler that focuses on low-noise regions (high log-SNR values). Aims to improve the learning of fine details.

To use this, specify `qinglong_flux` or `qinglong_qwen` for `--timestep_sampling`.

```bash
accelerate launch ... \
    --timestep_sampling qinglong_flux \
    --logit_mean -6.0 \
    --logit_std 2.0
```

Following is the distribution of the qinglong flux sampler:

![Distribution of qinglong flux sampler](qinglong_distribution.png)

<details>
<summary>æ—¥æœ¬èª</summary>

ã“ã‚Œã¯ã€ã‚¹ã‚¿ã‚¤ãƒ«ã®å­¦ç¿’ã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã€ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ã®å†ç¾æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ãŸã‚ã«ã€3ã¤ã®ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã§ã™ã€‚Style-Friendly SNR Samplerã«ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ã•ã‚ŒãŸå®Ÿé¨“çš„ãªæ©Ÿèƒ½ã§ã™ã€‚PR [#407](https://github.com/kohya-ss/musubi-tuner/pull/407) ã§ sdbds (Qing Long) æ°ã«ã‚ˆã‚Šææ¡ˆã•ã‚Œã¾ã—ãŸã€‚

å„å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã„ã¦ã€ãƒãƒƒãƒå†…ã®å„ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦ã€ã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã•ã‚ŒãŸæ¯”ç‡ã«åŸºã¥ãä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ãŒé¸æŠã•ã‚Œã¾ã™ã€‚

1.  **flux_shift ã¾ãŸã¯ qwen_shift (80%)**: é«˜è§£åƒåº¦ãƒ¢ãƒ‡ãƒ«å‘ã‘ã®æ¨™æº–çš„ãªã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã€‚å…¨ä½“çš„ãªå®‰å®šæ€§ã‚’é‡è¦–ã—ã¾ã™ã€‚
2.  **logsnr (7.5%)**: Style-Friendlyã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã€‚ã‚¹ã‚¿ã‚¤ãƒ«ã®å­¦ç¿’ã‚’é‡è¦–ã—ã¾ã™ã€‚
3.  **logsnr2 (12.5%)**: ä½ãƒã‚¤ã‚ºé ˜åŸŸï¼ˆé«˜ã„log-SNRå€¤ï¼‰ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã€‚ç´°éƒ¨ã®ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«å­¦ç¿’ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¾ã™ã€‚

ä½¿ç”¨ã™ã‚‹ã«ã¯ã€`--timestep_sampling` ã« `qinglong_flux` ã¾ãŸã¯ `qinglong_qwen` ã‚’æŒ‡å®šã—ã¾ã™ã€‚

</details>

## Specify time step range for training / å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ç¯„å›²ã®æŒ‡å®š

You can specify the range of timesteps for training. This is useful for focusing the training on a specific part of the diffusion process.

- `--min_timestep`: Specifies the minimum timestep for training (0-999, default: 0).
- `--max_timestep`: Specifies the maximum timestep for training (1-1000, default: 1000).
- `--preserve_distribution_shape`: If specified, it constrains timestep sampling to the `[min_timestep, max_timestep]` range using rejection sampling, which preserves the original distribution shape. By default, the `[0, 1]` range is scaled, which can distort the distribution. This option is only effective when `timestep_sampling` is not 'sigma'.

<details>
<summary>æ—¥æœ¬èª</summary>

å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ç¯„å›²ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹å®šã®éƒ¨åˆ†ã«å­¦ç¿’ã‚’é›†ä¸­ã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

- `--min_timestep`: å­¦ç¿’æ™‚ã®æœ€å°ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆ0-999ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0ï¼‰ã€‚
- `--max_timestep`: å­¦ç¿’æ™‚ã®æœ€å¤§ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆ1-1000ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰ã€‚
- `--preserve_distribution_shape`: æŒ‡å®šã™ã‚‹ã¨ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æ£„å´ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¡ä»¶ã«åˆã‚ãªã„ã‚‚ã®ã‚’æ¨ã¦ã‚‹ï¼‰ã‚’ç”¨ã„ã¦ `[min_timestep, max_timestep]` ã®ç¯„å›²ã«åˆ¶ç´„ã—ã€å…ƒã®åˆ†å¸ƒå½¢çŠ¶ã‚’ä¿æŒã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€`[0, 1]` ã®ç¯„å›²ãŒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ãŸã‚ã€åˆ†å¸ƒãŒæ­ªã‚€å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `timestep_sampling` ãŒ 'sigma' ä»¥å¤–ã®å ´åˆã«ã®ã¿æœ‰åŠ¹ã§ã™ã€‚
</details>

### Example / è¨˜è¿°ä¾‹

To train only on the latter half of the timesteps (more detailed part) / ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å¾ŒåŠï¼ˆã‚ˆã‚Šè©³ç´°ãªéƒ¨åˆ†ï¼‰ã®ã¿ã‚’å­¦ç¿’ã™ã‚‹å ´åˆ:

```bash
--min_timestep 500 --max_timestep 1000
```

To train only on the first half of the timesteps (more structural part) / ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å‰åŠï¼ˆã‚ˆã‚Šæ§‹é€ çš„ãªéƒ¨åˆ†ï¼‰ã®ã¿ã‚’å­¦ç¿’ã™ã‚‹å ´åˆ:

```bash
--min_timestep 0 --max_timestep 500
```

To train on a specific range while preserving the sampling distribution shape / ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åˆ†å¸ƒã®å½¢çŠ¶ã‚’ç¶­æŒã—ã¤ã¤ç‰¹å®šã®ç¯„å›²ã§å­¦ç¿’ã™ã‚‹å ´åˆ:

```bash
--min_timestep 200 --max_timestep 800 --preserve_distribution_shape
```

### Actual distribution shape / å®Ÿéš›ã®åˆ†å¸ƒå½¢çŠ¶

You can visualize the distribution shape of the timesteps with `--show_timesteps image` (or console) option. The distribution shape is determined by the `--min_timestep`, `--max_timestep`, and `--preserve_distribution_shape` options.

In the following examples, the discrete flow shift is set to 3.0.

When `--min_timestep` and `--max_timestep` are not specified, the distribution shape is as follows:

![no_timestep](./shift_3.png)

When `--min_timestep 500` and `--max_timestep 100` are specified, and `--preserve_distribution_shape` is not specified, the distribution shape is as follows:

![timestep_500_1000](./shift_3_500_1000.png)

When `--min_timestep 500` and `--max_timestep 100` are specified, and `--preserve_distribution_shape` is specified, the distribution shape is as follows:

![timestep_500_1000_preserve](./shift_3_500_1000_preserve.png)

<details>
<summary>æ—¥æœ¬èª</summary>

ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†å¸ƒå½¢çŠ¶ã¯ã€`--show_timesteps image`ï¼ˆã¾ãŸã¯consoleï¼‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ç¢ºèªã§ãã¾ã™ã€‚åˆ†å¸ƒå½¢çŠ¶ã¯ã€`--min_timestep`ã€`--max_timestep`ã€ãŠã‚ˆã³ `--preserve_distribution_shape` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦æ±ºã¾ã‚Šã¾ã™ã€‚

ä¸Šã®å›³ã¯ãã‚Œãã‚Œã€é›¢æ•£ãƒ•ãƒ­ãƒ¼ã‚·ãƒ•ãƒˆãŒ3.0ã®ã¨ãã€

1. `--min_timestep` ã¨ `--max_timestep` ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
2. `--min_timestep 500` ã¨ `--max_timestep 1000` ãŒæŒ‡å®šã•ã‚Œã€`--preserve_distribution_shape` ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
3. `--min_timestep 500` ã¨ `--max_timestep 1000` ãŒæŒ‡å®šã•ã‚Œã€`--preserve_distribution_shape` ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ

ã®åˆ†å¸ƒå½¢çŠ¶ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
</details>

## Timestep Bucketing for Uniform Sampling / å‡ä¸€ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãŸã‚ã®Timestep Bucketing

This feature is experimental.

When training with a small dataset or for a few epochs, the random sampling of timesteps can be biased, potentially leading to unstable training. To mitigate this, timestep bucketing ensures a more uniform distribution of timesteps throughout the training process.

This feature works as follows:

1. At the beginning of each epoch, it prepares a pool of timesteps equal to the number of items in the dataset for that epoch. These timesteps are calculated as follows:
   - A specified number of buckets is created. Each bucket represents an equal interval of the `[0, 1]` range (e.g., with 5 buckets, the ranges are `[0, 0.2]`, `[0.2, 0.4]`, ... `[0.8, 1.0]`).
   - Each bucket is filled with an equal number of randomly generated timesteps within its range.
   - The number of timesteps in each bucket is calculated as "number of dataset items Ã· number of buckets".

2. All timesteps from all buckets are then combined and shuffled.
3. During training, instead of generating a random timestep for each item, one is drawn from this pre-shuffled pool.

This ensures that the model sees a balanced distribution of timesteps in each epoch, which can improve training stability, especially for LoRA training or when using small datasets.

This feature is enabled by specifying `--num_timestep_buckets`.

<details>
<summary>æ—¥æœ¬èª</summary>

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå°ã•ã„å ´åˆã‚„å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ãŒå°‘ãªã„å ´åˆã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ä¹±æ•°ã«åã‚ŠãŒç”Ÿã˜ã‚‹ã“ã¨ã§ã€å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚Timestep Bucketingæ©Ÿèƒ½ã¯ã€ã“ã®å•é¡Œã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã§ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚ˆã‚Šå‡ä¸€ã«åˆ†å¸ƒã™ã‚‹ã‚ˆã†èª¿æ•´ã—ã¾ã™ã€‚

ã“ã®æ©Ÿèƒ½ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ï¼š

1. å„ã‚¨ãƒãƒƒã‚¯ã®é–‹å§‹æ™‚ã«ã€ã‚ã‚‰ã‹ã˜ã‚ãã®ã‚¨ãƒãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä»¶æ•°ã¨åŒã˜æ•°ã®ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’æº–å‚™ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚

    - æŒ‡å®šã•ã‚ŒãŸæ•°ã®ãƒã‚±ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã™ã€‚å„ãƒã‚±ãƒƒãƒˆã¯ `[0, 1]` ã®ç¯„å›²ã‚’ç­‰åˆ†ã—ãŸåŒºé–“ã‚’è¡¨ã—ã¾ã™ï¼ˆä¾‹ï¼š5ãƒã‚±ãƒƒãƒˆã®å ´åˆã€`[0, 0.2]`ã€`[0.2, 0.4]` ... `[0.8, 1.0]`ï¼‰ã€‚
    - å„ãƒã‚±ãƒƒãƒˆã«ã€ãã®ç¯„å›²å†…ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’é…ç½®ã—ã¾ã™ã€‚
    - ãã‚Œãã‚Œã®ãƒã‚±ãƒƒãƒˆã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ä»¶æ•°ã¯ã€ã€Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä»¶æ•°Ã·ãƒã‚±ãƒƒãƒˆæ•°ã€ã§è¨ˆç®—ã•ã‚Œã¾ã™ã€‚

2. ã™ã¹ã¦ã®ãƒã‚±ãƒƒãƒˆã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãŒçµåˆã•ã‚Œã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã•ã‚Œã¾ã™ã€‚
3. å­¦ç¿’æ™‚ã«ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç”Ÿæˆã™ã‚‹ä»£ã‚ã‚Šã«ã€ã“ã®äº‹å‰ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã•ã‚ŒãŸãƒ—ãƒ¼ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãŒå–ã‚Šå‡ºã•ã‚Œã¾ã™ã€‚

ã“ã‚Œã«ã‚ˆã‚Šã€å„ã‚¨ãƒãƒƒã‚¯ã§ãƒ¢ãƒ‡ãƒ«ãŒãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†å¸ƒã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ãªã‚Šã€ç‰¹ã«LoRAã®å­¦ç¿’ã‚„å°è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹éš›ã®å­¦ç¿’ã®å®‰å®šæ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚

ã“ã®æ©Ÿèƒ½ã¯ `--num_timestep_buckets` ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§æœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚

</details>

### How to use / ä½¿ç”¨æ–¹æ³•

Specify the number of buckets with the `--num_timestep_buckets` option. A value of 2 or more enables this feature. If not specified, it is disabled.

The community research is required to determine the optimal value, but starting with a value between `4` and `10` may be a good idea.

<details>
<summary>æ—¥æœ¬èª</summary>

`--num_timestep_buckets` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒã‚±ãƒƒãƒˆæ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚2ä»¥ä¸Šã®å€¤ã‚’æŒ‡å®šã™ã‚‹ã¨ã“ã®æ©Ÿèƒ½ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç„¡åŠ¹ã§ã™ã€‚

æœ€é©ãªå€¤ã«é–¢ã—ã¦ã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ¤œè¨¼ãŒå¿…è¦ã§ã™ãŒã€`4` ã‹ã‚‰ `10` ç¨‹åº¦ã®å€¤ã‹ã‚‰å§‹ã‚ã‚‹ã¨è‰¯ã„ã¨æ€ã‚ã‚Œã¾ã™ã€‚

</details>

### Example / è¨˜è¿°ä¾‹

```bash
accelerate launch ... \
    --num_timestep_buckets 5
```

### Notes / æ³¨æ„ç‚¹

- This feature may not work as expected when training with both high and low noise models simultaneously in `wan_train_network.py` (`--dit_high_noise` option) or when `--preserve_distribution_shape` is specified. Because the way timesteps are handled will differ in these cases.

    Specifically, instead of selecting from pre-configured timestep buckets, the process involves determining buckets on-demand and generating random timesteps within the range each bucket covers. Therefore, the uniform sampling effect may not be achieved, but some improvement can be expected compared to completely random generation (within the `[0, 1]` range).

<details>
<summary>æ—¥æœ¬èª</summary>

- `wan_train_network.py` ã§high/lowãƒã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã‚’åŒæ™‚ã«å­¦ç¿’ã™ã‚‹å ´åˆï¼ˆ`--dit_high_noise` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€ãŠã‚ˆã³ã€`--preserve_distribution_shape` ã‚’æŒ‡å®šã—ãŸå ´åˆã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®æ‰±ã„ãŒç•°ãªã‚‹ãŸã‚ã€ã“ã®æ©Ÿèƒ½ã¯æœŸå¾…é€šã‚Šã«å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

    å…·ä½“çš„ã«ã¯ã€ã‚ã‚‰ã‹ã˜ã‚è¨­å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒã‚±ãƒƒãƒˆã‹ã‚‰é¸æŠã•ã‚Œã‚‹ã®ã§ã¯ãªãã€éƒ½åº¦ã€ãƒã‚±ãƒ„ã®æ±ºå®šâ†’ç¯„å›²å†…ã§ã®ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ç”ŸæˆãŒè¡Œã‚ã‚Œã¾ã™ã€‚ã“ã®ãŸã‚ã€å‡ä¸€ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åŠ¹æœãŒå¾—ã‚‰ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€å®Œå…¨ãªãƒ©ãƒ³ãƒ€ãƒ ï¼ˆ`[0, 1]` ã®ç¯„å›²ã§ã®ç”Ÿæˆï¼‰ã«æ¯”ã¹ã‚‹ã¨ã€å¤šå°‘ã®æ”¹å–„ãŒè¦‹è¾¼ã¾ã‚Œã¾ã™ã€‚

</details>

## Schedule Free Optimizer / ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ•ãƒªãƒ¼ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶

[Schedule Free Optimizer](https://github.com/facebookresearch/schedule_free) is an optimizer that does not require a learning rate schedule.

The library is optional, so you can install it with `pip install schedulefree`.

Specify the optimizer with the `--optimizer_type` argument, using the format `package_name.ClassName`, for example: `--optimizer_type schedulefree.AdamWScheduleFree`.

You can specify multiple arguments for the optimizer using the `--optimizer_args` argument in the form `arg_name=value` (e.g., `--optimizer_args "weight_decay=0.01" "betas=(0.9,0.95)"`).

<details>
<summary>æ—¥æœ¬èª</summary>

[Schedule Free Optimizer](https://github.com/facebookresearch/schedule_free)ã¯ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å¿…è¦ã¨ã—ãªã„ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã§ã™ã€‚

ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãŸã‚ã€`pip install schedulefree` ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

`--optimizer_type`å¼•æ•°ã«ã€` --optimizer_type schedulefree.AdamWScheduleFree`ã®ã‚ˆã†ã«ã€`ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å.ã‚¯ãƒ©ã‚¹å`ã®å½¢å¼ã§æŒ‡å®šã—ã¾ã™ã€‚ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¸ã®å¼•æ•°ã¯ã€`--optimizer_args`ã«`å¼•æ•°å=å€¤`ã®å½¢ã§è¤‡æ•°æŒ‡å®šã§ãã¾ã™ï¼ˆä¾‹ï¼š`--optimizer_args "weight_decay=0.01" "betas=(0.9,0.95)"`ï¼‰ã€‚

</details>

## Custom LR Scheduler / ã‚«ã‚¹ã‚¿ãƒ LRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©

### Rex

The Rex scheduler was added in [PR #513](https://github.com/kohya-ss/musubi-tuner/pull/513). It is based on the paper [REX: Revisiting Budgeted Training with an Improved Schedule](https://arxiv.org/abs/2107.04197), and the implementation is based on the repository by [IvanVassi](https://github.com/IvanVassi/REX_LR).

It has two parameters, `rex_alpha` and `rex_beta`, with default values of 0.1 and 0.9, respectively. These parameters are based on the defaults in IvanVassi's repository. The values proposed in the paper are 0.5 and 0.5. You can also use `--lr_warmup_steps` (default is 0) and `--lr_scheduler_min_lr_ratio` (default is 0.01).

It is similar to the Polynomial Scheduler with power less than 1, but Rex has a more gradual decrease in learning rate. For the specific LR curve, refer to the explanation in PR #513.

It is enabled by specifying `--lr_scheduler rex`. You can specify the parameters with `--lr_scheduler_args`.

```bash
--lr_scheduler rex --lr_scheduler_args "rex_alpha=0.1" "rex_beta=0.9"
```

<details>
<summary>æ—¥æœ¬èª</summary>

Rexã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ [PR #513](https://github.com/kohya-ss/musubi-tuner/pull/513) ã§è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚è«–æ–‡ [REX: Revisiting Budgeted Training with an Improved Schedule](https://arxiv.org/abs/2107.04197) ã«åŸºã¥ã„ã¦ã„ãŸã‚‚ã®ã§ã€å®Ÿè£…ã¯ [IvanVassi](https://github.com/IvanVassi/REX_LR) æ°ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å…ƒã«ã—ã¦ã„ã¾ã™ã€‚

`rex_alpha`ã¨`rex_beta`ã®2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¡ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯ãã‚Œãã‚Œ0.1ã¨0.9ã§ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯IvanVassiæ°ã®ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚è«–æ–‡ã§æå”±ã•ã‚Œã¦ã„ã‚‹å€¤ã¯ãã‚Œãã‚Œ0.5ï¼0.5ã§ã™ã€‚ã¾ãŸã€`--lr_warmup_steps` ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0ï¼‰ãŠã‚ˆã³ `--lr_scheduler_min_lr_ratio` ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0.01ï¼‰ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚

powerã‚’1æœªæº€ã«è¨­å®šã—ãŸ Polynomial Scheduler ã«ä¼¼ã¦ã„ã¾ã™ãŒã€Rexã¯å­¦ç¿’ç‡ã®æ¸›å°‘ãŒã‚ˆã‚Šç·©ã‚„ã‹ã§ã™ã€‚å…·ä½“çš„ãªLRã®ã‚«ãƒ¼ãƒ–ã¯PR #513ã®èª¬æ˜ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--lr_scheduler rex`ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§æœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚`--lr_scheduler_args`ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

```bash
--lr_scheduler rex --lr_scheduler_args "rex_alpha=0.1" "rex_beta=0.9"
```

</details>