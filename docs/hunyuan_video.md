> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# HunyuanVideo

## Overview / æ¦‚è¦

This document describes the usage of the [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) architecture within the Musubi Tuner framework. HunyuanVideo is a video generation model that supports text-to-video generation.

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Musubi Tunerãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å†…ã§ã®[HunyuanVideo](https://github.com/Tencent/HunyuanVideo)ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä½¿ç”¨æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚HunyuanVideoã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚
</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

There are two ways to download the model.

### Use the Official HunyuanVideo Model / å…¬å¼HunyuanVideoãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†

Download the model following the [official README](https://github.com/Tencent/HunyuanVideo/blob/main/ckpts/README.md) and place it in your chosen directory with the following structure:

```
  ckpts
    â”œâ”€â”€hunyuan-video-t2v-720p
    â”‚  â”œâ”€â”€transformers
    â”‚  â”œâ”€â”€vae
    â”œâ”€â”€text_encoder
    â”œâ”€â”€text_encoder_2
    â”œâ”€â”€...
```

### Using ComfyUI Models for Text Encoder / Text Encoderã«ComfyUIæä¾›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†

This method is easier.

For DiT and VAE, use the HunyuanVideo models.

From https://huggingface.co/tencent/HunyuanVideo/tree/main/hunyuan-video-t2v-720p/transformers, download [mp_rank_00_model_states.pt](https://huggingface.co/tencent/HunyuanVideo/resolve/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt) and place it in your chosen directory.

(Note: The fp8 model on the same page is unverified.)

If you are training with `--fp8_base`, you can use `mp_rank_00_model_states_fp8.safetensors` from [here](https://huggingface.co/kohya-ss/HunyuanVideo-fp8_e4m3fn-unofficial) instead of `mp_rank_00_model_states.pt`. (This file is unofficial and simply converts the weights to float8_e4m3fn.)

From https://huggingface.co/tencent/HunyuanVideo/tree/main/hunyuan-video-t2v-720p/vae, download [pytorch_model.pt](https://huggingface.co/tencent/HunyuanVideo/resolve/main/hunyuan-video-t2v-720p/vae/pytorch_model.pt) and place it in your chosen directory.

For the Text Encoder, use the models provided by ComfyUI. Refer to [ComfyUI's page](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/), from https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files/text_encoders, download `llava_llama3_fp16.safetensors` (Text Encoder 1, LLM) and `clip_l.safetensors` (Text Encoder 2, CLIP) and place them in your chosen directory.

(Note: The fp8 LLM model on the same page is unverified.)

<details>
<summary>æ—¥æœ¬èª</summary>

ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

### HunyuanVideoã®å…¬å¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã† 

[å…¬å¼ã®README](https://github.com/Tencent/HunyuanVideo/blob/main/ckpts/README.md)ã‚’å‚è€ƒã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä»¥ä¸‹ã®ã‚ˆã†ã«é…ç½®ã—ã¾ã™ã€‚

```
  ckpts
    â”œâ”€â”€hunyuan-video-t2v-720p
    â”‚  â”œâ”€â”€transformers
    â”‚  â”œâ”€â”€vae
    â”œâ”€â”€text_encoder
    â”œâ”€â”€text_encoder_2
    â”œâ”€â”€...
```

### Text Encoderã«ComfyUIæä¾›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†

ã“ã¡ã‚‰ã®æ–¹æ³•ã®æ–¹ãŒã‚ˆã‚Šç°¡å˜ã§ã™ã€‚DiTã¨VAEã®ãƒ¢ãƒ‡ãƒ«ã¯HumyuanVideoã®ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

https://huggingface.co/tencent/HunyuanVideo/tree/main/hunyuan-video-t2v-720p/transformers ã‹ã‚‰ã€[mp_rank_00_model_states.pt](https://huggingface.co/tencent/HunyuanVideo/resolve/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¾ã™ã€‚

ï¼ˆåŒã˜ãƒšãƒ¼ã‚¸ã«fp8ã®ãƒ¢ãƒ‡ãƒ«ã‚‚ã‚ã‚Šã¾ã™ãŒã€æœªæ¤œè¨¼ã§ã™ã€‚ï¼‰

`--fp8_base`ã‚’æŒ‡å®šã—ã¦å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€`mp_rank_00_model_states.pt`ã®ä»£ã‚ã‚Šã«ã€[ã“ã¡ã‚‰](https://huggingface.co/kohya-ss/HunyuanVideo-fp8_e4m3fn-unofficial)ã®`mp_rank_00_model_states_fp8.safetensors`ã‚’ä½¿ç”¨å¯èƒ½ã§ã™ã€‚ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯éå…¬å¼ã®ã‚‚ã®ã§ã€é‡ã¿ã‚’å˜ç´”ã«float8_e4m3fnã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚ï¼‰

ã¾ãŸã€https://huggingface.co/tencent/HunyuanVideo/tree/main/hunyuan-video-t2v-720p/vae ã‹ã‚‰ã€[pytorch_model.pt](https://huggingface.co/tencent/HunyuanVideo/resolve/main/hunyuan-video-t2v-720p/vae/pytorch_model.pt) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¾ã™ã€‚

Text Encoderã«ã¯ComfyUIæä¾›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚[ComyUIã®ãƒšãƒ¼ã‚¸](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)ã‚’å‚è€ƒã«ã€https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files/text_encoders ã‹ã‚‰ã€llava_llama3_fp16.safetensors ï¼ˆText Encoder 1ã€LLMï¼‰ã¨ã€clip_l.safetensors ï¼ˆText Encoder 2ã€CLIPï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¾ã™ã€‚

ï¼ˆåŒã˜ãƒšãƒ¼ã‚¸ã«fp8ã®LLMãƒ¢ãƒ‡ãƒ«ã‚‚ã‚ã‚Šã¾ã™ãŒã€å‹•ä½œæœªæ¤œè¨¼ã§ã™ã€‚ï¼‰

</details>

## Pre-caching / äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

### Latent Pre-caching / latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

Latent pre-caching is required. Create the cache using the following command:

If you have installed using pip:

```bash
python src/musubi_tuner/cache_latents.py --dataset_config path/to/toml --vae path/to/ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt --vae_chunk_size 32 --vae_tiling
```

If you have installed with `uv`, you can use `uv run --extra cu124` to run the script. If CUDA 12.8 is supported, `uv run --extra cu128` is also available. Other scripts can be run in the same way. (Note that the installation with `uv` is experimental. Feedback is welcome. If you encounter any issues, please use the pip-based installation.)

```bash
uv run --extra cu124 src/musubi_tuner/cache_latents.py --dataset_config path/to/toml --vae path/to/ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt --vae_chunk_size 32 --vae_tiling
```

For additional options, use `python src/musubi_tuner/cache_latents.py --help`.

If you're running low on VRAM, reduce `--vae_spatial_tile_sample_min_size` to around 128 and lower the `--batch_size`.

Use `--debug_mode image` to display dataset images and captions in a new window, or `--debug_mode console` to display them in the console (requires `ascii-magic`). 

With `--debug_mode video`, images or videos will be saved in the cache directory (please delete them after checking). The bitrate of the saved video is set to 1Mbps for preview purposes. The images decoded from the original video (not degraded) are used for the cache (for training).

When `--debug_mode` is specified, the actual caching process is not performed.

By default, cache files not included in the dataset are automatically deleted. You can still keep cache files as before by specifying `--keep_cache`.

<details>
<summary>æ—¥æœ¬èª</summary>

latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ï¼ˆpipã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆï¼‰

```bash
python src/musubi_tuner/cache_latents.py --dataset_config path/to/toml --vae path/to/ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt --vae_chunk_size 32 --vae_tiling
```

uvã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå ´åˆã¯ã€`uv run --extra cu124 python src/musubi_tuner/cache_latents.py ...`ã®ã‚ˆã†ã«ã€`uv run --extra cu124`ã‚’å…ˆé ­ã«ã¤ã‘ã¦ãã ã•ã„ã€‚CUDA 12.8ã«å¯¾å¿œã—ã¦ã„ã‚‹å ´åˆã¯ã€`uv run --extra cu128`ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚‚åŒæ§˜ã§ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯`python src/musubi_tuner/cache_latents.py --help`ã§ç¢ºèªã§ãã¾ã™ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--vae_spatial_tile_sample_min_size`ã‚’128ç¨‹åº¦ã«æ¸›ã‚‰ã—ã€`--batch_size`ã‚’å°ã•ãã—ã¦ãã ã•ã„ã€‚

`--debug_mode image` ã‚’æŒ‡å®šã™ã‚‹ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒæ–°è¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚`--debug_mode console`ã§ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤ºã•ã‚Œã¾ã™ï¼ˆ`ascii-magic`ãŒå¿…è¦ï¼‰ã€‚

`--debug_mode video`ã§ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç”»åƒã¾ãŸã¯å‹•ç”»ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼ˆç¢ºèªå¾Œã€å‰Šé™¤ã—ã¦ãã ã•ã„ï¼‰ã€‚å‹•ç”»ã®ãƒ“ãƒƒãƒˆãƒ¬ãƒ¼ãƒˆã¯ç¢ºèªç”¨ã«ä½ãã—ã¦ã‚ã‚Šã¾ã™ã€‚å®Ÿéš›ã«ã¯å…ƒå‹•ç”»ã®ç”»åƒãŒå­¦ç¿’ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

`--debug_mode`æŒ‡å®šæ™‚ã¯ã€å®Ÿéš›ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œãªã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã¾ã™ã€‚`--keep_cache`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ®‹ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

</details>

### Text Encoder Output Pre-caching / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

Text Encoder output pre-caching is required. Create the cache using the following command:

```bash
python src/musubi_tuner/cache_text_encoder_outputs.py --dataset_config path/to/toml  --text_encoder1 path/to/ckpts/text_encoder --text_encoder2 path/to/ckpts/text_encoder_2 --batch_size 16
```

or for uv:

```bash
uv run --extra cu124 src/musubi_tuner/cache_text_encoder_outputs.py --dataset_config path/to/toml  --text_encoder1 path/to/ckpts/text_encoder --text_encoder2 path/to/ckpts/text_encoder_2 --batch_size 16
```

For additional options, use `python src/musubi_tuner/cache_text_encoder_outputs.py --help`.

Adjust `--batch_size` according to your available VRAM.

For systems with limited VRAM (less than ~16GB), use `--fp8_llm` to run the LLM in fp8 mode.

By default, cache files not included in the dataset are automatically deleted. You can still keep cache files as before by specifying `--keep_cache`.

<details>
<summary>æ—¥æœ¬èª</summary>

Text Encoderå‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

```bash
python src/musubi_tuner/cache_text_encoder_outputs.py --dataset_config path/to/toml  --text_encoder1 path/to/ckpts/text_encoder --text_encoder2 path/to/ckpts/text_encoder_2 --batch_size 16
```

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯`python src/musubi_tuner/cache_text_encoder_outputs.py --help`ã§ç¢ºèªã§ãã¾ã™ã€‚

`--batch_size`ã¯VRAMã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆï¼ˆ16GBç¨‹åº¦æœªæº€ã®å ´åˆï¼‰ã¯ã€`--fp8_llm`ã‚’æŒ‡å®šã—ã¦ã€fp8ã§LLMã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œãªã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã¾ã™ã€‚`--keep_cache`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ®‹ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

</details>

## Training / å­¦ç¿’

Start training using the following command (input as a single line):

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py 
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt 
    --dataset_config path/to/toml --sdpa --mixed_precision bf16 --fp8_base 
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing 
    --max_data_loader_n_workers 2 --persistent_data_loader_workers 
    --network_module networks.lora --network_dim 32 
    --timestep_sampling shift --discrete_flow_shift 7.0 
    --max_train_epochs 16 --save_every_n_epochs 1 --seed 42
    --output_dir path/to/output_dir --output_name name-of-lora
```

or for uv:

```bash
uv run --extra cu124 accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py 
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt 
    --dataset_config path/to/toml --sdpa --mixed_precision bf16 --fp8_base 
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing 
    --max_data_loader_n_workers 2 --persistent_data_loader_workers 
    --network_module networks.lora --network_dim 32 
    --timestep_sampling shift --discrete_flow_shift 7.0 
    --max_train_epochs 16 --save_every_n_epochs 1 --seed 42
    --output_dir path/to/output_dir --output_name name-of-lora
```

If the details of the image are not learned well, try lowering the discrete flow shift to around 3.0.

The training settings are still experimental. Appropriate learning rates, training steps, timestep distribution, loss weighting, etc. are not yet known. Feedback is welcome.

For additional options, use `python src/musubi_tuner/hv_train_network.py --help` (note that many options are unverified).

Specifying `--fp8_base` runs DiT in fp8 mode. Without this flag, mixed precision data type will be used. fp8 can significantly reduce memory consumption but may impact output quality. If `--fp8_base` is not specified, 24GB or more VRAM is recommended. Use `--blocks_to_swap` as needed.

If you're running low on VRAM, use `--blocks_to_swap` to offload some blocks to CPU. Maximum value is 36.

(The idea of block swap is based on the implementation by 2kpr. Thanks again to 2kpr.)

Use `--sdpa` for PyTorch's scaled dot product attention. Use `--flash_attn` for [FlashAttention](https://github.com/Dao-AILab/flash-attention). Use `--xformers` for xformers, but specify `--split_attn` when using xformers. `--sage_attn` for SageAttention, but SageAttention is not yet supported for training, so it raises a ValueError.

`--split_attn` processes attention in chunks. Speed may be slightly reduced, but VRAM usage is slightly reduced.

The format of LoRA trained is the same as `sd-scripts`.

You can also specify the range of timesteps 
with `--min_timestep` and `--max_timestep`. See [advanced configuration](../advanced_config.md#specify-time-step-range-for-training--å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ç¯„å›²ã®æŒ‡å®š) for details.

`--show_timesteps` can be set to `image` (requires `matplotlib`) or `console` to display timestep distribution and loss weighting during training. (When using `flux_shift` and `qwen_shift`, the distribution will be for images with a resolution of 1024x1024.)

You can record logs during training. Refer to [Save and view logs in TensorBoard format](../advanced_config.md#save-and-view-logs-in-tensorboard-format--tensorboardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§).

For PyTorch Dynamo optimization, refer to [this document](../advanced_config.md#pytorch-dynamo-optimization-for-model-training--ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã‘ã‚‹pytorch-dynamoã®æœ€é©åŒ–).

For sample image generation during training, refer to [this document](../sampling_during_training.md). For advanced configuration, refer to [this document](../advanced_config.md).

<details>
<summary>æ—¥æœ¬èª</summary>

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆå®Ÿéš›ã«ã¯ä¸€è¡Œã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼‰ã€‚

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py 
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt 
    --dataset_config path/to/toml --sdpa --mixed_precision bf16 --fp8_base 
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing 
    --max_data_loader_n_workers 2 --persistent_data_loader_workers 
    --network_module networks.lora --network_dim 32 
    --timestep_sampling shift --discrete_flow_shift 7.0 
    --max_train_epochs 16 --save_every_n_epochs 1 --seed 42
    --output_dir path/to/output_dir --output_name name-of-lora
```

__æ›´æ–°__ï¼šã‚µãƒ³ãƒ—ãƒ«ã®å­¦ç¿’ç‡ã‚’1e-3ã‹ã‚‰2e-4ã«ã€`--timestep_sampling`ã‚’`sigmoid`ã‹ã‚‰`shift`ã«ã€`--discrete_flow_shift`ã‚’1.0ã‹ã‚‰7.0ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚ã‚ˆã‚Šé«˜é€Ÿãªå­¦ç¿’ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ãŒç”˜ããªã‚‹å ´åˆã¯ã€discrete flow shiftã‚’3.0ç¨‹åº¦ã«ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ã€‚

ãŸã ã€é©åˆ‡ãªå­¦ç¿’ç‡ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€timestepsã®åˆ†å¸ƒã€loss weightingãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ä»¥å‰ã¨ã—ã¦ä¸æ˜ãªç‚¹ãŒæ•°å¤šãã‚ã‚Šã¾ã™ã€‚æƒ…å ±æä¾›ã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯`python src/musubi_tuner/hv_train_network.py --help`ã§ç¢ºèªã§ãã¾ã™ï¼ˆãŸã ã—å¤šãã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯å‹•ä½œæœªç¢ºèªã§ã™ï¼‰ã€‚

`--fp8_base`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€DiTãŒfp8ã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚æœªæŒ‡å®šæ™‚ã¯mixed precisionã®ãƒ‡ãƒ¼ã‚¿å‹ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚fp8ã¯å¤§ããæ¶ˆè²»ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã§ãã¾ã™ãŒã€å“è³ªã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚`--fp8_base`ã‚’æŒ‡å®šã—ãªã„å ´åˆã¯VRAM 24GBä»¥ä¸Šã‚’æ¨å¥¨ã—ã¾ã™ã€‚ã¾ãŸå¿…è¦ã«å¿œã˜ã¦`--blocks_to_swap`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--blocks_to_swap`ã‚’æŒ‡å®šã—ã¦ã€ä¸€éƒ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚æœ€å¤§36ãŒæŒ‡å®šã§ãã¾ã™ã€‚

ï¼ˆblock swapã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯2kpræ°ã®å®Ÿè£…ã«åŸºã¥ãã‚‚ã®ã§ã™ã€‚2kpræ°ã«ã‚ã‚‰ãŸã‚ã¦æ„Ÿè¬ã—ã¾ã™ã€‚ï¼‰

`--sdpa`ã§PyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ã—ã¾ã™ã€‚`--flash_attn`ã§[FlashAttention]:(https://github.com/Dao-AILab/flash-attention)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚`--xformers`ã§xformersã®åˆ©ç”¨ã‚‚å¯èƒ½ã§ã™ãŒã€xformersã‚’ä½¿ã†å ´åˆã¯`--split_attn`ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚`--sage_attn`ã§SageAttentionã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€SageAttentionã¯ç¾æ™‚ç‚¹ã§ã¯å­¦ç¿’ã«æœªå¯¾å¿œã®ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

`--split_attn`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€attentionã‚’åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚é€Ÿåº¦ãŒå¤šå°‘ä½ä¸‹ã—ã¾ã™ãŒã€VRAMä½¿ç”¨é‡ã¯ã‚ãšã‹ã«æ¸›ã‚Šã¾ã™ã€‚

å­¦ç¿’ã•ã‚Œã‚‹LoRAã®å½¢å¼ã¯ã€`sd-scripts`ã¨åŒã˜ã§ã™ã€‚

`--min_timestep`ã¨`--max_timestep`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ç¯„å›²ã‚’æŒ‡å®šã§ãã¾ã™ã€‚è©³ç´°ã¯[é«˜åº¦ãªè¨­å®š](../advanced_config.md#specify-time-step-range-for-training--å­¦ç¿’æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ç¯„å›²ã®æŒ‡å®š)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--show_timesteps`ã«`image`ï¼ˆ`matplotlib`ãŒå¿…è¦ï¼‰ã¾ãŸã¯`console`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€å­¦ç¿’æ™‚ã®timestepsã®åˆ†å¸ƒã¨timestepsã”ã¨ã®loss weightingãŒç¢ºèªã§ãã¾ã™ã€‚ï¼ˆ`flux_shift`ã¨`qwen_shift`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ç”»åƒã®è§£åƒåº¦ãŒ1024x1024ã®å ´åˆã®åˆ†å¸ƒã«ãªã‚Šã¾ã™ã€‚ï¼‰

å­¦ç¿’æ™‚ã®ãƒ­ã‚°ã®è¨˜éŒ²ãŒå¯èƒ½ã§ã™ã€‚[TensorBoardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§](../advanced_config.md#save-and-view-logs-in-tensorboard-format--tensorboardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

PyTorch Dynamoã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’è¡Œã†å ´åˆã¯ã€[ã“ã¡ã‚‰](../advanced_config.md#pytorch-dynamo-optimization-for-model-training--ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã‘ã‚‹pytorch-dynamoã®æœ€é©åŒ–)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”»åƒç”Ÿæˆã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](../sampling_during_training.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ãã®ä»–ã®é«˜åº¦ãªè¨­å®šã«ã¤ã„ã¦ã¯[ã“ã¡ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](../advanced_config.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

### Merging LoRA Weights / LoRAã®é‡ã¿ã®ãƒãƒ¼ã‚¸

```bash
python src/musubi_tuner/merge_lora.py \
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt \
    --lora_weight path/to/lora.safetensors \
    --save_merged_model path/to/merged_model.safetensors \
    --device cpu \
    --lora_multiplier 1.0
```

or for uv:

```bash
uv run --extra cu124 src/musubi_tuner/merge_lora.py \
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt \
    --lora_weight path/to/lora.safetensors \
    --save_merged_model path/to/merged_model.safetensors \
    --device cpu \
    --lora_multiplier 1.0
```

Specify the device to perform the calculation (`cpu` or `cuda`, etc.) with `--device`. Calculation will be faster if `cuda` is specified.

Specify the LoRA weights to merge with `--lora_weight` and the multiplier for the LoRA weights with `--lora_multiplier`. Multiple values can be specified, and the number of values must match.

<details>
<summary>æ—¥æœ¬èª</summary>

```bash
python src/musubi_tuner/merge_lora.py \
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt \
    --lora_weight path/to/lora.safetensors \
    --save_merged_model path/to/merged_model.safetensors \
    --device cpu \
    --lora_multiplier 1.0
```

`--device`ã«ã¯è¨ˆç®—ã‚’è¡Œã†ãƒ‡ãƒã‚¤ã‚¹ï¼ˆ`cpu`ã¾ãŸã¯`cuda`ç­‰ï¼‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚`cuda`ã‚’æŒ‡å®šã™ã‚‹ã¨è¨ˆç®—ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚

`--lora_weight`ã«ã¯ãƒãƒ¼ã‚¸ã™ã‚‹LoRAã®é‡ã¿ã‚’ã€`--lora_multiplier`ã«ã¯LoRAã®é‡ã¿ã®ä¿‚æ•°ã‚’ã€ãã‚Œãã‚ŒæŒ‡å®šã—ã¦ãã ã•ã„ã€‚è¤‡æ•°å€‹ãŒæŒ‡å®šå¯èƒ½ã§ã€ä¸¡è€…ã®æ•°ã¯ä¸€è‡´ã•ã›ã¦ãã ã•ã„ã€‚

</details>

## Inference / æ¨è«–

Generate videos using the following command:

```bash
python src/musubi_tuner/hv_generate_video.py --fp8 --video_size 544 960 --video_length 5 --infer_steps 30 
    --prompt "A cat walks on the grass, realistic style."  --save_path path/to/save/dir --output_type both 
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt --attn_mode sdpa --split_attn
    --vae path/to/ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt 
    --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128 
    --text_encoder1 path/to/ckpts/text_encoder 
    --text_encoder2 path/to/ckpts/text_encoder_2 
    --seed 1234 --lora_multiplier 1.0 --lora_weight path/to/lora.safetensors
```

or for uv:

```bash
uv run --extra cu124 src/musubi_tuner/hv_generate_video.py --fp8 --video_size 544 960 --video_length 5 --infer_steps 30 
    --prompt "A cat walks on the grass, realistic style."  --save_path path/to/save/dir --output_type both 
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt --attn_mode sdpa --split_attn
    --vae path/to/ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt 
    --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128 
    --text_encoder1 path/to/ckpts/text_encoder 
    --text_encoder2 path/to/ckpts/text_encoder_2 
    --seed 1234 --lora_multiplier 1.0 --lora_weight path/to/lora.safetensors
```

For additional options, use `python src/musubi_tuner/hv_generate_video.py --help`.

Specifying `--fp8` runs DiT in fp8 mode. fp8 can significantly reduce memory consumption but may impact output quality.

`--fp8_fast` option is also available for faster inference on RTX 40x0 GPUs. This option requires `--fp8` option. 

If you're running low on VRAM, use `--blocks_to_swap` to offload some blocks to CPU. Maximum value is 38.

For `--attn_mode`, specify either `flash`, `torch`, `sageattn`, `xformers`, or `sdpa` (same as `torch`). These correspond to FlashAttention, scaled dot product attention, SageAttention, and xformers, respectively. Default is `torch`. SageAttention is effective for VRAM reduction.

Specifing `--split_attn` will process attention in chunks. Inference with SageAttention is expected to be about 10% faster.

For `--output_type`, specify either `both`, `latent`, `video` or `images`. `both` outputs both latents and video. Recommended to use `both` in case of Out of Memory errors during VAE processing. You can specify saved latents with `--latent_path` and use `--output_type video` (or `images`) to only perform VAE decoding.

`--seed` is optional. A random seed will be used if not specified.

`--video_length` should be specified as "a multiple of 4 plus 1".

`--flow_shift` can be specified to shift the timestep (discrete flow shift). The default value when omitted is 7.0, which is the recommended value for 50 inference steps. In the HunyuanVideo paper, 7.0 is recommended for 50 steps, and 17.0 is recommended for less than 20 steps (e.g. 10).

By specifying `--video_path`, video2video inference is possible. Specify a video file or a directory containing multiple image files (the image files are sorted by file name and used as frames). An error will occur if the video is shorter than `--video_length`. You can specify the strength with `--strength`. It can be specified from 0 to 1.0, and the larger the value, the greater the change from the original video.

Note that video2video inference is experimental.

`--compile` option enables PyTorch's compile feature (experimental). Requires triton. On Windows, also requires Visual C++ build tools installed and PyTorch>=2.6.0 (Visual C++ build tools is also required). You can pass arguments to the compiler with `--compile_args`.

The `--compile` option takes a long time to run the first time, but speeds up on subsequent runs.

You can save the DiT model after LoRA merge with the `--save_merged_model` option. Specify `--save_merged_model path/to/merged_model.safetensors`. Note that inference will not be performed when this option is specified.

<details>
<summary>æ—¥æœ¬èª</summary>

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

```bash
python src/musubi_tuner/hv_generate_video.py --fp8 --video_size 544 960 --video_length 5 --infer_steps 30 
    --prompt "A cat walks on the grass, realistic style."  --save_path path/to/save/dir --output_type both 
    --dit path/to/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt --attn_mode sdpa --split_attn
    --vae path/to/ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt 
    --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128 
    --text_encoder1 path/to/ckpts/text_encoder 
    --text_encoder2 path/to/ckpts/text_encoder_2 
    --seed 1234 --lora_multiplier 1.0 --lora_weight path/to/lora.safetensors
```

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯`python src/musubi_tuner/hv_generate_video.py --help`ã§ç¢ºèªã§ãã¾ã™ã€‚

`--fp8`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€DiTãŒfp8ã§æ¨è«–ã•ã‚Œã¾ã™ã€‚fp8ã¯å¤§ããæ¶ˆè²»ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã§ãã¾ã™ãŒã€å“è³ªã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

RTX 40x0ã‚·ãƒªãƒ¼ã‚ºã®GPUã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€`--fp8_fast`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€é«˜é€Ÿæ¨è«–ãŒå¯èƒ½ã§ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯ã€`--fp8`ã‚‚æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--blocks_to_swap`ã‚’æŒ‡å®šã—ã¦ã€ä¸€éƒ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚æœ€å¤§38ãŒæŒ‡å®šã§ãã¾ã™ã€‚

`--attn_mode`ã«ã¯`flash`ã€`torch`ã€`sageattn`ã€`xformers`ã¾ãŸã¯`sdpa`ï¼ˆ`torch`æŒ‡å®šæ™‚ã¨åŒã˜ï¼‰ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ãã‚Œãã‚ŒFlashAttentionã€scaled dot product attentionã€SageAttentionã€xformersã«å¯¾å¿œã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`torch`ã§ã™ã€‚SageAttentionã¯VRAMã®å‰Šæ¸›ã«æœ‰åŠ¹ã§ã™ã€‚

`--split_attn`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€attentionã‚’åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚SageAttentionåˆ©ç”¨æ™‚ã§10%ç¨‹åº¦ã®é«˜é€ŸåŒ–ãŒè¦‹è¾¼ã¾ã‚Œã¾ã™ã€‚

`--output_type`ã«ã¯`both`ã€`latent`ã€`video`ã€`images`ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚`both`ã¯latentã¨å‹•ç”»ã®ä¸¡æ–¹ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚VAEã§Out of Memoryã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆã«å‚™ãˆã¦ã€`both`ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚`--latent_path`ã«ä¿å­˜ã•ã‚ŒãŸlatentã‚’æŒ‡å®šã—ã€`--output_type video` ï¼ˆã¾ãŸã¯`images`ï¼‰ã¨ã—ã¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€VAEã®decodeã®ã¿ã‚’è¡Œãˆã¾ã™ã€‚

`--seed`ã¯çœç•¥å¯èƒ½ã§ã™ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ãªã‚·ãƒ¼ãƒ‰ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

`--video_length`ã¯ã€Œ4ã®å€æ•°+1ã€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

`--flow_shift`ã«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚·ãƒ•ãƒˆå€¤ï¼ˆdiscrete flow shiftï¼‰ã‚’æŒ‡å®šå¯èƒ½ã§ã™ã€‚çœç•¥æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯7.0ã§ã€ã“ã‚Œã¯æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒ50ã®æ™‚ã®æ¨å¥¨å€¤ã§ã™ã€‚HunyuanVideoã®è«–æ–‡ã§ã¯ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°50ã®å ´åˆã¯7.0ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°20æœªæº€ï¼ˆ10ãªã©ï¼‰ã§17.0ãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

`--video_path`ã«èª­ã¿è¾¼ã‚€å‹•ç”»ã‚’æŒ‡å®šã™ã‚‹ã¨ã€video2videoã®æ¨è«–ãŒå¯èƒ½ã§ã™ã€‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã‹ã€è¤‡æ•°ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¥ã£ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚½ãƒ¼ãƒˆã•ã‚Œã€å„ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦ç”¨ã„ã‚‰ã‚Œã¾ã™ï¼‰ã€‚`--video_length`ã‚ˆã‚Šã‚‚çŸ­ã„å‹•ç”»ã‚’æŒ‡å®šã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™ã€‚`--strength`ã§å¼·åº¦ã‚’æŒ‡å®šã§ãã¾ã™ã€‚0~1.0ã§æŒ‡å®šã§ãã€å¤§ãã„ã»ã©å…ƒã®å‹•ç”»ã‹ã‚‰ã®å¤‰åŒ–ãŒå¤§ãããªã‚Šã¾ã™ã€‚

ãªãŠvideo2videoæ¨è«–ã®å‡¦ç†ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚

`--compile`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§PyTorchã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ï¼ˆå®Ÿé¨“çš„æ©Ÿèƒ½ï¼‰ã€‚tritonã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ã¾ãŸã€Windowsã§ã¯Visual C++ build toolsãŒå¿…è¦ã§ã€ã‹ã¤PyTorch>=2.6.0ã§ã®ã¿å‹•ä½œã—ã¾ã™ã€‚`--compile_args`ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®å¼•æ•°ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

`--compile`ã¯åˆå›å®Ÿè¡Œæ™‚ã«ã‹ãªã‚Šã®æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã€2å›ç›®ä»¥é™ã¯é«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚

`--save_merged_model`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã€LoRAãƒãƒ¼ã‚¸å¾Œã®DiTãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã§ãã¾ã™ã€‚`--save_merged_model path/to/merged_model.safetensors`ã®ã‚ˆã†ã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ãªãŠã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨æ¨è«–ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚

</details>

### Inference with SkyReels V1 / SkyReels V1ã§ã®æ¨è«–

SkyReels V1 T2V and I2V models are supported (inference only). 

The model can be downloaded from [here](https://huggingface.co/Kijai/SkyReels-V1-Hunyuan_comfy). Many thanks to Kijai for providing the model. `skyreels_hunyuan_i2v_bf16.safetensors` is the I2V model, and `skyreels_hunyuan_t2v_bf16.safetensors` is the T2V model. The models other than bf16 are not tested (`fp8_e4m3fn` may work).

For T2V inference, add the following options to the inference command:

```bash
--guidance_scale 6.0 --embedded_cfg_scale 1.0 --negative_prompt "Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion" --split_uncond
```

SkyReels V1 seems to require a classfier free guidance (negative prompt).`--guidance_scale` is a guidance scale for the negative prompt. The recommended value is 6.0 from the official repository. The default is 1.0, it means no classifier free guidance.

`--embedded_cfg_scale` is a scale of the embedded guidance. The recommended value is 1.0 from the official repository (it may mean no embedded guidance).

`--negative_prompt` is a negative prompt for the classifier free guidance. The above sample is from the official repository. If you don't specify this, and specify `--guidance_scale` other than 1.0, an empty string will be used as the negative prompt.

`--split_uncond` is a flag to split the model call into unconditional and conditional parts. This reduces VRAM usage but may slow down inference. If `--split_attn` is specified, `--split_uncond` is automatically set.

You can also perform image2video inference with SkyReels V1 I2V model. Specify the image file path with `--image_path`. The image will be resized to the given `--video_size`.

```bash
--image_path path/to/image.jpg
``` 

<details>
<summary>æ—¥æœ¬èª</summary>

SkyReels V1ã®T2Vã¨I2Vãƒ¢ãƒ‡ãƒ«ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼ˆæ¨è«–ã®ã¿ï¼‰ã€‚

ãƒ¢ãƒ‡ãƒ«ã¯[ã“ã¡ã‚‰](https://huggingface.co/Kijai/SkyReels-V1-Hunyuan_comfy)ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¦ãã ã•ã£ãŸKijaiæ°ã«æ„Ÿè¬ã—ã¾ã™ã€‚`skyreels_hunyuan_i2v_bf16.safetensors`ãŒI2Vãƒ¢ãƒ‡ãƒ«ã€`skyreels_hunyuan_t2v_bf16.safetensors`ãŒT2Vãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚`bf16`ä»¥å¤–ã®å½¢å¼ã¯æœªæ¤œè¨¼ã§ã™ï¼ˆ`fp8_e4m3fn`ã¯å‹•ä½œã™ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼‰ã€‚

T2Væ¨è«–ã‚’è¡Œã†å ´åˆã€ä»¥ä¸‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¨è«–ã‚³ãƒãƒ³ãƒ‰ã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼š

```bash
--guidance_scale 6.0 --embedded_cfg_scale 1.0 --negative_prompt "Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion" --split_uncond
```

SkyReels V1ã¯classifier free guidanceï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã‚’å¿…è¦ã¨ã™ã‚‹ã‚ˆã†ã§ã™ã€‚`--guidance_scale`ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ã§ã™ã€‚å…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã®æ¨å¥¨å€¤ã¯6.0ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã§ã€ã“ã®å ´åˆã¯classifier free guidanceã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ç„¡è¦–ã•ã‚Œã¾ã™ï¼‰ã€‚

`--embedded_cfg_scale`ã¯åŸ‹ã‚è¾¼ã¿ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã™ã€‚å…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã®æ¨å¥¨å€¤ã¯1.0ã§ã™ï¼ˆåŸ‹ã‚è¾¼ã¿ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãªã—ã‚’æ„å‘³ã™ã‚‹ã¨æ€ã‚ã‚Œã¾ã™ï¼‰ã€‚

`--negative_prompt`ã¯ã„ã‚ã‚†ã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã™ã€‚ä¸Šè¨˜ã®ã‚µãƒ³ãƒ—ãƒ«ã¯å…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã®ã‚‚ã®ã§ã™ã€‚`--guidance_scale`ã‚’æŒ‡å®šã—ã€`--negative_prompt`ã‚’æŒ‡å®šã—ãªã‹ã£ãŸå ´åˆã¯ã€ç©ºæ–‡å­—åˆ—ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

`--split_uncond`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ã‚’uncondã¨condï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã«åˆ†å‰²ã—ã¾ã™ã€‚VRAMä½¿ç”¨é‡ãŒæ¸›ã‚Šã¾ã™ãŒã€æ¨è«–é€Ÿåº¦ã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚`--split_attn`ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€`--split_uncond`ã¯è‡ªå‹•çš„ã«æœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚

</details>

### Convert LoRA to another format / LoRAã®å½¢å¼ã®å¤‰æ›

You can convert LoRA to a format (presumed to be Diffusion-pipe) compatible with another inference environment (Diffusers, ComfyUI etc.) using the following command:

```bash
python src/musubi_tuner/convert_lora.py --input path/to/musubi_lora.safetensors --output path/to/another_format.safetensors --target other
```

or for uv:

```bash
uv run --extra cu124 src/musubi_tuner/convert_lora.py --input path/to/musubi_lora.safetensors --output path/to/another_format.safetensors --target other
```

Specify the input and output file paths with `--input` and `--output`, respectively.

Specify `other` for `--target`. Use `default` to convert from another format to the format of this repository.

<details>
<summary>æ—¥æœ¬èª</summary>

ä»–ã®æ¨è«–ç’°å¢ƒï¼ˆDiffusersã‚„ComfyUIï¼‰ã§ä½¿ç”¨å¯èƒ½ãªå½¢å¼ï¼ˆDiffusion-pipe ã¾ãŸã¯ Diffusers ã¨æ€ã‚ã‚Œã‚‹ï¼‰ã¸ã®å¤‰æ›ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§è¡Œãˆã¾ã™ã€‚

```bash
python src/musubi_tuner/convert_lora.py --input path/to/musubi_lora.safetensors --output path/to/another_format.safetensors --target other
```

`--input`ã¨`--output`ã¯ãã‚Œãã‚Œå…¥åŠ›ã¨å‡ºåŠ›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

`--target`ã«ã¯`other`ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚`default`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ä»–ã®å½¢å¼ã‹ã‚‰å½“ãƒªãƒã‚¸ãƒˆãƒªã®å½¢å¼ã«å¤‰æ›ã§ãã¾ã™ã€‚

</details>