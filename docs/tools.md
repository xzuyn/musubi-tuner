> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Tools

This document provides documentation for utility tools available in this project. 

## Table of Contents

- [LoRA Post-Hoc EMA merging / LoRAã®Post-Hoc EMAãƒãƒ¼ã‚¸](#lora-post-hoc-ema-merging--loraã®post-hoc-emaãƒãƒ¼ã‚¸)
- [Image Captioning with Qwen2.5-VL / Qwen2.5-VLã«ã‚ˆã‚‹ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ](#image-captioning-with-qwen25-vl--qwen25-vlã«ã‚ˆã‚‹ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ)

## LoRA Post-Hoc EMA merging / LoRAã®Post-Hoc EMAãƒãƒ¼ã‚¸

The LoRA Post-Hoc EMA (Exponential Moving Average) merging is a technique to combine multiple LoRA checkpoint files into a single, potentially more stable model. This method applies exponential moving average across multiple checkpoints sorted by modification time, with configurable decay rates.

The Post-Hoc EMA method works by:

1. Sorting checkpoint files by modification time (oldest to newest)
2. Using the oldest checkpoint as the base
3. Iteratively merging subsequent checkpoints with a decay rate (beta)
4. Optionally using linear interpolation between two beta values across the merge process

Pseudo-code for merging multiple checkpoints with beta=0.95 would look like this:

```
beta = 0.95
checkpoints = [checkpoint1, checkpoint2, checkpoint3]  # List of checkpoints
merged_weights = checkpoints[0]  # Use the first checkpoint as the base
for checkpoint in checkpoints[1:]:
    merged_weights = beta * merged_weights + (1 - beta) * checkpoint
```

### Key features:

- **Temporal ordering**: Automatically sorts files by modification time
- **Configurable decay rates**: Supports single beta value or linear interpolation between two beta values
- **Metadata preservation**: Maintains and updates metadata from the last checkpoint
- **Hash updating**: Recalculates model hashes for the merged weights
- **Dtype preservation**: Maintains original data types of tensors

### Usage

The LoRA Post-Hoc EMA merging is available as a standalone script:

```bash
python src/musubi_tuner/lora_post_hoc_ema.py checkpoint1.safetensors checkpoint2.safetensors checkpoint3.safetensors --output_file merged_lora.safetensors --beta 0.95
```

### Command line options:

```
path [path ...]
    List of paths to the LoRA weight files to merge

--beta BETA
    Decay rate for merging weights (default: 0.95)
    Higher values (closer to 1.0) give more weight to the accumulated average
    Lower values give more weight to the current checkpoint

--beta2 BETA2
    Second decay rate for linear interpolation (optional)
    If specified, the decay rate will linearly interpolate from beta to beta2
    across the merging process

--sigma_rel SIGMA_REL
    Relative sigma for Power Function EMA (optional, mutually exclusive with beta/beta2)
    This resolves the issue where the first checkpoint has a disproportionately large influence when beta is specified.
    If specified, beta is calculated using the Power Function EMA method from the paper:
    https://arxiv.org/pdf/2312.02696. This overrides beta and beta2.

--output_file OUTPUT_FILE
    Output file path for the merged weights (required)

--no_sort
    Disable sorting of checkpoint files (merge in specified order)
```

### Examples:

Basic usage with constant decay rate:
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_merged.safetensors \
    --beta 0.95
```

Using linear interpolation between two decay rates:
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_interpolated.safetensors \
    --beta 0.90 \
    --beta2 0.95
```

Using Power Function EMA with `sigma_rel`:
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_power_ema_merged.safetensors \
    --sigma_rel 0.2
```


#### betas for different Ïƒ-rel values:

![beta-sigma_rel-graph](./betas_for_sigma_rel.png)

### Recommended settings example (after training for 30 epochs, using  `--beta`)

If you're unsure which settings to try, start with the following "General Recommended Settings".

#### 1. General Recommended Settings (start with these combinations)

- **Target Epochs:** `15-30` (the latter half of training)
- **beta:** `0.9` (a balanced value)

#### 2. If training converged early

- **Situation:** Loss dropped early and stabilized afterwards.
- **Target Epochs:** `10-30` (from the epoch where loss stabilized to the end)
- **beta:** `0.95` (wider range, smoother)

#### 3. If you want to avoid overfitting

- **Situation:** In the latter part of training, generated results are too similar to training data.
- **Target Epochs:** `15-25` (focus on the peak performance range)
- **beta:** `0.8` (more emphasis on the latter part of the range while maintaining diversity)

**Note:** The optimal values may vary depending on the model and dataset. It's recommended to experiment with multiple `beta` values (e.g., 0.8, 0.9, 0.95) and compare the generated results.

### Recommended Settings Example (30 epochs training, using `--sigma_rel`)

When using `--sigma_rel`, the beta decay schedule is determined by the Power Function EMA method. Here are some starting points:

#### 1. General Recommended Settings
- **Target Epochs:** All epochs (from the first to the last).
- **sigma_rel:** `0.2` (a general starting point).

#### 2. If training converged early
- **Situation:** Loss dropped early and stabilized afterwards.
- **Target Epochs:** All epochs.
- **sigma_rel:** `0.25` (gives more weight to earlier checkpoints, suitable for early convergence).

#### 3. If you want to avoid overfitting
- **Situation:** In the latter part of training, generated results are too similar to training data.
- **Target Epochs:** From the first epoch, omitting the last few potentially overfitted epochs.
- **sigma_rel:** `0.15` (gives more weight to later (but not the very last) checkpoints, helping to mitigate overfitting from the final stages).

**Note:** The optimal `sigma_rel` value can depend on the dataset, model, and training duration. Experimentation is encouraged. Values typically range from 0.1 to 0.5. A graph showing the relationship between `sigma_rel` and the calculated `beta` values over epochs will be provided later to help understand its behavior.

### Notes:

- Files are automatically sorted by modification time, so the order in the command line doesn't matter
- The `--sigma_rel` option is mutually exclusive with `--beta` and `--beta2`. If `--sigma_rel` is provided, it will determine the beta values, and any provided `--beta` or `--beta2` will be ignored.
- All checkpoint files to be merged should be from the same training run, saved per epoch or step
    - Merging is possible if shapes match, but may not work correctly as Post Hoc EMA
- All checkpoint files must have the same alpha value
- The merged model will have updated hash values in its metadata 
- The metadata of the merged model will be taken from the last checkpoint, with only the hash value recalculated
- Non-float tensors (long, int, bool, etc.) are not merged and will use the first checkpoint's values
- Processing is done in float32 precision to maintain numerical stability during merging. The original data types are preserved when saving

<details>
<summary>æ—¥æœ¬èª</summary>

LoRA Post-Hoc EMAï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰ãƒãƒ¼ã‚¸ã¯ã€è¤‡æ•°ã®LoRAãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å˜ä¸€ã®ã€ã‚ˆã‚Šå®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã«çµåˆã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€ä¿®æ­£æ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰ã•ã‚ŒãŸè¤‡æ•°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«å¯¾ã—ã¦æŒ‡å®šã•ã‚ŒãŸæ¸›è¡°ç‡ã§æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’é©ç”¨ã—ã¾ã™ã€‚æ¸›è¡°ç‡ã¯æŒ‡å®šå¯èƒ½ã§ã™ã€‚

Post-Hoc EMAæ–¹æ³•ã®å‹•ä½œï¼š

1. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£æ™‚åˆ»é †ï¼ˆå¤ã„ã‚‚ã®ã‹ã‚‰æ–°ã—ã„ã‚‚ã®ã¸ï¼‰ã«ã‚½ãƒ¼ãƒˆ
2. æœ€å¤ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
3. æ¸›è¡°ç‡ï¼ˆbetaï¼‰ã‚’ä½¿ã£ã¦å¾Œç¶šã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’åå¾©çš„ã«ãƒãƒ¼ã‚¸
4. ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã€ãƒãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§2ã¤ã®ãƒ™ãƒ¼ã‚¿å€¤é–“ã®ç·šå½¢è£œé–“ã‚’ä½¿ç”¨

ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼šè¤‡æ•°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’beta=0.95ã§ãƒãƒ¼ã‚¸ã™ã‚‹å ´åˆã€æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚

```
beta = 0.95
checkpoints = [checkpoint1, checkpoint2, checkpoint3]  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
merged_weights = checkpoints[0]  # æœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
for checkpoint in checkpoints[1:]:
    merged_weights = beta * merged_weights + (1 - beta) * checkpoint
```

### ä¸»ãªç‰¹å¾´ï¼š

- **æ™‚ç³»åˆ—é †åºä»˜ã‘**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£æ™‚åˆ»ã§è‡ªå‹•çš„ã«ã‚½ãƒ¼ãƒˆ
- **è¨­å®šå¯èƒ½ãªæ¸›è¡°ç‡**: å˜ä¸€ã®ãƒ™ãƒ¼ã‚¿å€¤ã¾ãŸã¯2ã¤ã®ãƒ™ãƒ¼ã‚¿å€¤é–“ã®ç·šå½¢è£œé–“ã‚’ã‚µãƒãƒ¼ãƒˆ
- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿æŒ**: æœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¶­æŒãƒ»æ›´æ–°
- **ãƒãƒƒã‚·ãƒ¥æ›´æ–°**: ãƒãƒ¼ã‚¸ã•ã‚ŒãŸé‡ã¿ã®ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’å†è¨ˆç®—
- **ãƒ‡ãƒ¼ã‚¿å‹ä¿æŒ**: ãƒ†ãƒ³ã‚½ãƒ«ã®å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¶­æŒ

### ä½¿ç”¨æ³•

LoRA Post-Hoc EMAãƒãƒ¼ã‚¸ã¯ç‹¬ç«‹ã—ãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã—ã¦æä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼š

```bash
python src/musubi_tuner/lora_post_hoc_ema.py checkpoint1.safetensors checkpoint2.safetensors checkpoint3.safetensors --output_file merged_lora.safetensors --beta 0.95
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼š

```
path [path ...]
    ãƒãƒ¼ã‚¸ã™ã‚‹LoRAé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

--beta BETA
    é‡ã¿ãƒãƒ¼ã‚¸ã®ãŸã‚ã®æ¸›è¡°ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š0.95ï¼‰
    é«˜ã„å€¤ï¼ˆ1.0ã«è¿‘ã„ï¼‰ã¯ç´¯ç©å¹³å‡ã«ã‚ˆã‚Šå¤§ããªé‡ã¿ã‚’ä¸ãˆã‚‹ï¼ˆå¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’é‡è¦–ï¼‰
    ä½ã„å€¤ã¯ç¾åœ¨ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚Šå¤§ããªé‡ã¿ã‚’ä¸ãˆã‚‹

--beta2 BETA2
    ç·šå½¢è£œé–“ã®ãŸã‚ã®ç¬¬2æ¸›è¡°ç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    æŒ‡å®šã•ã‚ŒãŸå ´åˆã€æ¸›è¡°ç‡ã¯ãƒãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§betaã‹ã‚‰beta2ã¸ç·šå½¢è£œé–“ã•ã‚Œã‚‹

--sigma_rel SIGMA_REL
    Power Function EMAã®ãŸã‚ã®ç›¸å¯¾ã‚·ã‚°ãƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€beta/beta2ã¨åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“ï¼‰
    betaã‚’æŒ‡å®šã—ãŸå ´åˆã®ã€æœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒç›¸å¯¾çš„ã«å¤§ããªå½±éŸ¿ã‚’æŒã¤æ¬ ç‚¹ã‚’è§£æ±ºã—ã¾ã™
    æŒ‡å®šã•ã‚ŒãŸå ´åˆã€betaã¯æ¬¡ã®è«–æ–‡ã«åŸºã¥ã„ã¦Power Function EMAæ³•ã§è¨ˆç®—ã•ã‚Œã¾ã™ï¼š
    https://arxiv.org/pdf/2312.02696. ã“ã‚Œã«ã‚ˆã‚Šbetaã¨beta2ãŒä¸Šæ›¸ãã•ã‚Œã¾ã™ã€‚

--output_file OUTPUT_FILE
    ãƒãƒ¼ã‚¸ã•ã‚ŒãŸé‡ã¿ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå¿…é ˆï¼‰

--no_sort
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚½ãƒ¼ãƒˆã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆæŒ‡å®šã—ãŸé †åºã§ãƒãƒ¼ã‚¸ï¼‰
```

### ä¾‹ï¼š

å®šæ•°æ¸›è¡°ç‡ã§ã®åŸºæœ¬çš„ãªä½¿ç”¨æ³•ï¼š
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_merged.safetensors \
    --beta 0.95
```

2ã¤ã®æ¸›è¡°ç‡é–“ã®ç·šå½¢è£œé–“ã‚’ä½¿ç”¨ï¼š
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_interpolated.safetensors \
    --beta 0.90 \
    --beta2 0.95
```

`ã‚·ã‚°ãƒ_rel`ã‚’ä½¿ç”¨ã—ãŸPower Function EMAï¼š
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_power_ema_merged.safetensors \
    --sigma_rel 0.2
```

### æ¨å¥¨è¨­å®šã®ä¾‹ (30ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã—ã€ `--beta`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ)

ã©ã®è¨­å®šã‹ã‚‰è©¦ã›ã°è‰¯ã„ã‹åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ã€ã¾ãšä»¥ä¸‹ã®ã€Œ**ä¸€èˆ¬çš„ãªæ¨å¥¨è¨­å®š**ã€ã‹ã‚‰å§‹ã‚ã¦ã¿ã¦ãã ã•ã„ã€‚

#### 1. ä¸€èˆ¬çš„ãªæ¨å¥¨è¨­å®š (ã¾ãšè©¦ã™ã¹ãçµ„ã¿åˆã‚ã›)

- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** `15-30` (å­¦ç¿’ã®å¾ŒåŠåŠåˆ†)
- **beta:** `0.9` (ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå€¤)

#### 2. æ—©æœŸã«å­¦ç¿’ãŒåæŸã—ãŸå ´åˆ

- **çŠ¶æ³:** lossãŒæ—©ã„æ®µéšã§ä¸‹ãŒã‚Šã€ãã®å¾Œã¯å®‰å®šã—ã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** `10-30` (lossãŒå®‰å®šã—å§‹ã‚ãŸã‚¨ãƒãƒƒã‚¯ã‹ã‚‰æœ€å¾Œã¾ã§)
- **beta:** `0.95` (å¯¾è±¡ç¯„å›²ãŒåºƒã„ã®ã§ã€ã‚ˆã‚Šæ»‘ã‚‰ã‹ã«ã™ã‚‹)

#### 3. éå­¦ç¿’ã‚’é¿ã‘ãŸã„å ´åˆ

- **çŠ¶æ³:** å­¦ç¿’ã®æœ€å¾Œã®æ–¹ã§ã€ç”ŸæˆçµæœãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ä¼¼ã™ãã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** `15-25` (æ€§èƒ½ã®ãƒ”ãƒ¼ã‚¯ã¨æ€ã‚ã‚Œã‚‹ç¯„å›²ã«çµã‚‹)
- **beta:** `0.8` (ç¯„å›²ã®çµ‚ç›¤ã‚’é‡è¦–ã—ã¤ã¤ã€å¤šæ§˜æ€§ã‚’æ®‹ã™)

**ãƒ’ãƒ³ãƒˆ:** æœ€é©ãªå€¤ã¯ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚è¤‡æ•°ã®`beta`ï¼ˆä¾‹: 0.8, 0.9, 0.95ï¼‰ã‚’è©¦ã—ã¦ã€ç”Ÿæˆçµæœã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

### æ¨å¥¨è¨­å®šã®ä¾‹ (30ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã—ã€ `--sigma_rel`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ)

`--sigma_rel` ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€betaã®æ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯Power Function EMAæ³•ã«ã‚ˆã£ã¦æ±ºå®šã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã¯ã„ãã¤ã‹ã®é–‹å§‹ç‚¹ã§ã™ã€‚

#### 1. ä¸€èˆ¬çš„ãªæ¨å¥¨è¨­å®š
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** å…¨ã¦ã®ã‚¨ãƒãƒƒã‚¯ï¼ˆæœ€åˆã‹ã‚‰æœ€å¾Œã¾ã§ï¼‰
- **sigma_rel:** `0.2` ï¼ˆä¸€èˆ¬çš„ãªé–‹å§‹ç‚¹ï¼‰

#### 2. æ—©æœŸã«å­¦ç¿’ãŒåæŸã—ãŸå ´åˆ
- **çŠ¶æ³:** lossãŒæ—©ã„æ®µéšã§ä¸‹ãŒã‚Šã€ãã®å¾Œã¯å®‰å®šã—ã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** å…¨ã¦ã®ã‚¨ãƒãƒƒã‚¯
- **sigma_rel:** `0.25` ï¼ˆåˆæœŸã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«é‡ãã‚’ç½®ããŸã‚ã€æ—©æœŸåæŸã«é©ã—ã¦ã„ã¾ã™ï¼‰

#### 3. éå­¦ç¿’ã‚’é¿ã‘ãŸã„å ´åˆ
- **çŠ¶æ³:** å­¦ç¿’ã®æœ€å¾Œã®æ–¹ã§ã€ç”ŸæˆçµæœãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ä¼¼ã™ãã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** æœ€åˆã®ã‚¨ãƒãƒƒã‚¯ã‹ã‚‰ã€éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚‹æœ€å¾Œã®æ•°ã‚¨ãƒãƒƒã‚¯ã‚’é™¤å¤–
- **sigma_rel:** `0.15` ï¼ˆçµ‚ç›¤ï¼ˆãŸã ã—æœ€å¾Œã®æœ€å¾Œã§ã¯ãªã„ï¼‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«é‡ãã‚’ç½®ãã€æœ€çµ‚æ®µéšã§ã®éå­¦ç¿’ã‚’è»½æ¸›ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ï¼‰

**ãƒ’ãƒ³ãƒˆ:** æœ€é©ãª `sigma_rel` ã®å€¤ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ¢ãƒ‡ãƒ«ã€å­¦ç¿’æœŸé–“ã«ã‚ˆã£ã¦ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚å®Ÿé¨“ã‚’æ¨å¥¨ã—ã¾ã™ã€‚å€¤ã¯é€šå¸¸0.1ã‹ã‚‰0.5ã®ç¯„å›²ã§ã™ã€‚`sigma_rel` ã¨ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®è¨ˆç®—ã•ã‚ŒãŸ `beta` å€¤ã®é–¢ä¿‚ã‚’ç¤ºã™ã‚°ãƒ©ãƒ•ã¯ã€ãã®æŒ™å‹•ã‚’ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã‚ˆã†å¾Œã»ã©æä¾›ã™ã‚‹äºˆå®šã§ã™ã€‚

### æ³¨æ„ç‚¹ï¼š

- ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿®æ­£æ™‚åˆ»ã§è‡ªå‹•çš„ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã‚‹ãŸã‚ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ã®é †åºã¯é–¢ä¿‚ã‚ã‚Šã¾ã›ã‚“
- `--sigma_rel`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯`--beta`ãŠã‚ˆã³`--beta2`ã¨ç›¸äº’ã«æ’ä»–çš„ã§ã™ã€‚`--sigma_rel`ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã€ãã‚ŒãŒãƒ™ãƒ¼ã‚¿å€¤ã‚’æ±ºå®šã—ã€æŒ‡å®šã•ã‚ŒãŸ`--beta`ã¾ãŸã¯`--beta2`ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚
- ãƒãƒ¼ã‚¸ã™ã‚‹å…¨ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ã²ã¨ã¤ã®å­¦ç¿’ã§ã€ã‚¨ãƒãƒƒã‚¯ã”ã¨ã€ã¾ãŸã¯ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    - å½¢çŠ¶ãŒä¸€è‡´ã—ã¦ã„ã‚Œã°ãƒãƒ¼ã‚¸ã¯ã§ãã¾ã™ãŒã€Post Hoc EMAã¨ã—ã¦ã¯æ­£ã—ãå‹•ä½œã—ã¾ã›ã‚“
- alphaå€¤ã¯ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§åŒã˜ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
- ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã€æœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚‚ã®ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒãƒƒã‚·ãƒ¥å€¤ã®ã¿ãŒå†è¨ˆç®—ã•ã‚Œã¾ã™
- æµ®å‹•å°æ•°ç‚¹ä»¥å¤–ã®ã€longã€intã€boolãªã©ã®ãƒ†ãƒ³ã‚½ãƒ«ã¯ãƒãƒ¼ã‚¸ã•ã‚Œã¾ã›ã‚“ï¼ˆæœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚‚ã®ãŒä½¿ç”¨ã•ã‚Œã¾ã™ï¼‰
- ãƒãƒ¼ã‚¸ä¸­ã®æ•°å€¤å®‰å®šæ€§ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã«float32ç²¾åº¦ã§è¨ˆç®—ã•ã‚Œã¾ã™ã€‚ä¿å­˜æ™‚ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ãŒç¶­æŒã•ã‚Œã¾ã™

</details>

## Image Captioning with Qwen2.5-VL / Qwen2.5-VLã«ã‚ˆã‚‹ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ

The `caption_images_by_qwen_vl.py` script automatically generates captions for a directory of images using a fine-tuned Qwen2.5-VL model. It's designed to help prepare datasets for training by creating captions from the images themselves.

The Qwen2.5-VL model used in Qwen-Image is not confirmed to be the same as the original Qwen2.5-VL-Instruct model, but it appears to work for caption generation based on the tests conducted.

<details>
<summary>æ—¥æœ¬èª</summary>

`caption_images_by_qwen_vl.py`ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€Qwen2.5-VLãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒã«å¯¾ã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚ç”»åƒè‡ªä½“ã‹ã‚‰ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã‚’æ”¯æ´ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

Qwen-Imageã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹Qwen2.5-VLãƒ¢ãƒ‡ãƒ«ã¯ã€å…ƒã®Qwen2.5-VL-Instructãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã‹ã©ã†ã‹ä¸æ˜ã§ã™ãŒã€è©¦ã—ãŸç¯„å›²ã§ã¯ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã§ã™ã€‚

</details>

### Arguments

-   `--image_dir` (required): Path to the directory containing the images to be captioned.
-   `--model_path` (required): Path to the Qwen2.5-VL model. See [here](./qwen_image.md#download-the-model--ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰) for instructions.
-   `--output_file` (optional): Path to the output JSONL file. This is required if `--output_format` is `jsonl`.
-   `--max_new_tokens` (optional, default: 1024): The maximum number of new tokens to generate for each caption.
-   `--prompt` (optional, default: see script): A custom prompt to use for caption generation. You can use `\n` for newlines.
-   `--max_size` (optional, default: 1280): The maximum size of the image. Images are resized to fit within a `max_size` x `max_size` area while maintaining aspect ratio.
-   `--fp8_vl` (optional, flag): If specified, the Qwen2.5-VL model is loaded in fp8 precision for lower memory usage.
-   `--output_format` (optional, default: `jsonl`): The output format. Can be `jsonl` to save all captions in a single JSONL file, or `text` to save a separate `.txt` file for each image.

`--max_size` can be reduced to decrease the image size passed to the VLM. This can reduce the memory usage of the VLM, but may also decrease the quality of the generated captions.

The default prompt is defined in the [source file](./src/musubi_tuner/caption_images_by_qwen_vl.py). It is based on the [Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324).

<details>
<summary>æ—¥æœ¬èª</summary>

-   `--image_dir` (å¿…é ˆ): ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ç”»åƒãŒå«ã¾ã‚Œã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚
-   `--model_path` (å¿…é ˆ): Qwen2.5-VLãƒ¢ãƒ‡ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚è©³ç´°ã¯[ã“ã¡ã‚‰](./qwen_image.md#download-the-model--ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
-   `--output_file` (ä»»æ„): å‡ºåŠ›å…ˆã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚`--output_format`ãŒ`jsonl`ã®å ´åˆã«å¿…é ˆã§ã™ã€‚
-   `--max_new_tokens` (ä»»æ„, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1024): å„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã§ç”Ÿæˆã™ã‚‹æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§æ•°ã€‚
-   `--prompt` (ä»»æ„, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…å‚ç…§): ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚`\n`ã§æ”¹è¡Œã‚’æŒ‡å®šã§ãã¾ã™ã€‚
-   `--max_size` (ä»»æ„, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1280): ç”»åƒã®æœ€å¤§ã‚µã‚¤ã‚ºã€‚ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ãŸã¾ã¾ã€ç”»åƒã®åˆè¨ˆãƒ”ã‚¯ã‚»ãƒ«æ•°ãŒ`max_size` x `max_size`ã®é ˜åŸŸã«åã¾ã‚‹ã‚ˆã†ã«ãƒªã‚µã‚¤ã‚ºã•ã‚Œã¾ã™ã€‚
-   `--fp8_vl` (ä»»æ„, ãƒ•ãƒ©ã‚°): æŒ‡å®šã•ã‚ŒãŸå ´åˆã€Qwen2.5-VLãƒ¢ãƒ‡ãƒ«ãŒfp8ç²¾åº¦ã§èª­ã¿è¾¼ã¾ã‚Œã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå‰Šæ¸›ã•ã‚Œã¾ã™ã€‚
-   `--output_format` (ä»»æ„, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `jsonl`): å‡ºåŠ›å½¢å¼ã€‚`jsonl`ã‚’æŒ‡å®šã™ã‚‹ã¨ã™ã¹ã¦ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒå˜ä¸€ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã•ã‚Œã€`text`ã‚’æŒ‡å®šã™ã‚‹ã¨ç”»åƒã”ã¨ã«å€‹åˆ¥ã®`.txt`ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

`--max_size` ã‚’å°ã•ãã™ã‚‹ã¨VLMã«æ¸¡ã•ã‚Œã‚‹ç”»åƒã‚µã‚¤ã‚ºãŒå°ã•ããªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€VLMã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå‰Šæ¸›ã•ã‚Œã¾ã™ãŒã€ç”Ÿæˆã•ã‚Œã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®å“è³ªãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€[ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«](./src/musubi_tuner/caption_images_by_qwen_vl.py)å†…ã§å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚[Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324)ã‚’å‚è€ƒã«ã—ãŸã‚‚ã®ã§ã™ã€‚

</details>

### Usage Examples

**1. Basic Usage (JSONL Output)**

```bash
python src/musubi_tuner/caption_images_by_qwen_vl.py \
  --image_dir /path/to/images \
  --model_path /path/to/qwen_model.safetensors \
  --output_file /path/to/captions.jsonl
```

**2. Text File Output**

This will create a `.txt` file with the same name as each image in the `/path/to/images` directory.

```bash
python src/musubi_tuner/caption_images_by_qwen_vl.py \
  --image_dir /path/to/images \
  --model_path /path/to/qwen_model.safetensors \
  --output_format text
```

**3. Advanced Usage (fp8, Custom Prompt, and Max Size)**

```bash
python src/musubi_tuner/caption_images_by_qwen_vl.py \
  --image_dir /path/to/images \
  --model_path /path/to/qwen_model.safetensors \
  --output_file /path/to/captions.jsonl \
  --fp8_vl \
  --max_size 1024 \
  --prompt "A detailed and descriptive caption for this image is:\n"
```
